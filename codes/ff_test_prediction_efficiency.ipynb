{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-27T04:48:12.913589800Z",
     "start_time": "2024-08-27T04:47:59.391529700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from matplotlib) (4.50.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from matplotlib) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from matplotlib) (6.3.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.18.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:  tensor([[-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
      "        [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
      "        [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
      "        ...,\n",
      "        [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
      "        [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
      "        [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102]])\n",
      "Training Data Label:  tensor([9, 0, 0,  ..., 3, 0, 5])\n",
      "Positive Data:  tensor([[-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
      "        [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
      "        [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
      "        ...,\n",
      "        [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
      "        [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
      "        [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102]],\n",
      "       device='cuda:0')\n",
      "Negative Data:  tensor([[-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
      "        [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
      "        [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
      "        ...,\n",
      "        [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
      "        [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102],\n",
      "        [-0.8102, -0.8102, -0.8102,  ..., -0.8102, -0.8102, -0.8102]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 1 ...\n",
      "SHAPE: torch.Size([10, 60000])\n",
      "Training Accuracy:  0.6818666458129883\n",
      "SHAPE: torch.Size([10, 10000])\n",
      "Testing Accuracy:  0.675000011920929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "!pip install tqdm\n",
    "!pip install matplotlib\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST, FashionMNIST, CIFAR10\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Lambda\n",
    "\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from torch import Tensor\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import Adam\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "number_of_epochs = 20\n",
    "\n",
    "hidden_layer_neurons = 1000\n",
    "\n",
    "class Layer(nn.Linear):\n",
    "    # hebbian_weights_layer_one_zeroth = nn.Parameter(torch.ones(10, hidden_layer_neurons).cuda())\n",
    "    # hebbian_weights_layer_two_zeroth = nn.Parameter(torch.ones(10, hidden_layer_neurons).cuda())\n",
    "    hebbian_weights_layer_one = nn.Parameter(torch.ones(10, hidden_layer_neurons).cuda())\n",
    "    # hebbian_weights_layer_one_quadra = nn.Parameter(torch.ones(10, hidden_layer_neurons).cuda())\n",
    "    hebbian_weights_layer_two = nn.Parameter(torch.ones(10, hidden_layer_neurons).cuda())\n",
    "    # hebbian_weights_layer_two_quadra = nn.Parameter(torch.ones(10, hidden_layer_neurons).cuda())\n",
    "    # hebbian_weights_layer_one_third = nn.Parameter(torch.ones(10, hidden_layer_neurons).cuda())\n",
    "    # hebbian_weights_layer_two_third = nn.Parameter(torch.ones(10, hidden_layer_neurons).cuda())\n",
    "    # \n",
    "    def __init__(self, in_features, out_features, bias=True, device=None, d_type=None, is_hinge_loss=False):\n",
    "        super().__init__(in_features, out_features, bias, device, d_type)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.learning_rate = 0.02\n",
    "        self.optimizer = Adam(self.parameters(), lr=self.learning_rate)\n",
    "        self.threshold = 2.0\n",
    "        self.num_of_epochs = number_of_epochs\n",
    "        self.is_hinge_loss = is_hinge_loss\n",
    "        self.hebbian_optimizer = Adam([Layer.hebbian_weights_layer_two, Layer.hebbian_weights_layer_one], lr=0.02)\n",
    "        \n",
    "        \n",
    "    def compute_hebbian_activity(self, labels, values, layer_num):\n",
    "        \n",
    "        \n",
    "        if layer_num == 0:\n",
    "            hebbian_value = torch.mm(values.pow(2) * self.layer_weights[layer_num], self.hebbian_weights_layer_one.T) * labels\n",
    "        else:\n",
    "            hebbian_value = torch.mm(values, self.hebbian_weights_layer_two.T) * labels\n",
    "        return hebbian_value.mean(1).cuda()\n",
    "    \n",
    "    \n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        normalized_input = input / (input.norm(2, 1, keepdim=True) + 1e-4)\n",
    "        output = torch.mm(normalized_input, self.weight.T) + self.bias.unsqueeze(0)\n",
    "        return self.activation(output)\n",
    "\n",
    "    def balanced_loss(self, positive_goodness, negative_goodness, difference_sum, alpha=8.0):\n",
    "        delta = positive_goodness.cuda() - negative_goodness.cuda() + difference_sum.cuda()\n",
    "        per_instance_loss = torch.log(1 + torch.exp(-alpha * delta))\n",
    "        return per_instance_loss.mean()  \n",
    "\n",
    "\n",
    "    def exponential_hinge_loss(self, positive_goodness, negative_goodness, delta=1.0, is_second_phase=False):\n",
    "        if is_second_phase:\n",
    "            threshold = self.threshold * 2\n",
    "        else:\n",
    "            threshold = self.threshold\n",
    "        positive_loss = torch.exp(torch.clamp(delta - (positive_goodness - threshold), min=0)) - 1\n",
    "        negative_loss = torch.exp(torch.clamp(delta - (threshold - negative_goodness), min=0)) - 1\n",
    "        return torch.cat([positive_loss, negative_loss]).mean()\n",
    "\n",
    "\n",
    "\n",
    "    def plot_goodness(self, positive_goodness, negative_goodness, difference):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(positive_goodness, label='Positive Goodness', color='b')\n",
    "        plt.plot(negative_goodness, label='Negative Goodness', color='r')\n",
    "        plt.plot(difference, label='Difference Goodness', color='g')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Goodness Value')\n",
    "        plt.title('Change in Goodness During Training')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def get_difference_from_other_layers(self, layer_num):\n",
    "        difference_sum = torch.zeros(Network.layer_differences.shape[0])\n",
    "        for i in range(Network.layer_differences.shape[1]):\n",
    "            if i != layer_num:\n",
    "                difference_sum = difference_sum.cuda().add_(Network.layer_differences[:, i].cuda())\n",
    "        return difference_sum\n",
    "        \n",
    "    def train_layer(self, positive_input, negative_input, layer_num):\n",
    "        positive_goodness_history = []\n",
    "        negative_goodness_history = []\n",
    "        difference_history = []\n",
    "        difference_sum = self.get_difference_from_other_layers(layer_num).cuda()\n",
    "        \n",
    "        \n",
    "        for _ in range(number_of_epochs):\n",
    "            positive_output = self.forward(positive_input)  # Shape: [batch_size, 500]\n",
    "            negative_output = self.forward(negative_input)\n",
    "            \n",
    "            # First Layer\n",
    "            if layer_num == 0:\n",
    "                # positive_goodness = (torch.mm(Network.positive_labels, Layer.hebbian_weights_layer_one_zeroth) * positive_output + torch.mm(Network.positive_labels, Layer.hebbian_weights_layer_one) * positive_output.pow(2) + torch.mm(Network.positive_labels, Layer.hebbian_weights_layer_one_quadra) * positive_output.pow(4) + torch.mm(Network.positive_labels, Layer.hebbian_weights_layer_one_third) * positive_output.pow(3)).mean(1)\n",
    "                positive_goodness = (torch.mm(Network.positive_labels, Layer.hebbian_weights_layer_one) * positive_output.pow(2)).mean(1)\n",
    "                # negative_goodness = (torch.mm(Network.negative_labels, Layer.hebbian_weights_layer_one_zeroth) * negative_output + torch.mm(Network.negative_labels, Layer.hebbian_weights_layer_one) * negative_output.pow(2) + torch.mm(Network.negative_labels, Layer.hebbian_weights_layer_one_quadra) * negative_output.pow(4) + torch.mm(Network.negative_labels, Layer.hebbian_weights_layer_one_third) * negative_output.pow(3)).mean(1)\n",
    "                negative_goodness = (torch.mm(Network.negative_labels, Layer.hebbian_weights_layer_one) * negative_output.pow(2)).mean(1)\n",
    "            else:\n",
    "                # Second Layer\n",
    "                # positive_goodness = (torch.mm(Network.positive_labels, Layer.hebbian_weights_layer_two_zeroth) * positive_output + torch.mm(Network.positive_labels, Layer.hebbian_weights_layer_two) * positive_output.pow(2) + torch.mm(Network.positive_labels, Layer.hebbian_weights_layer_two_quadra) * positive_output.pow(4) + torch.mm(Network.positive_labels, Layer.hebbian_weights_layer_two_third) * positive_output.pow(3)).mean(1)\n",
    "                positive_goodness = (torch.mm(Network.positive_labels, Layer.hebbian_weights_layer_two) * positive_output.pow(2)).mean(1)\n",
    "                # negative_goodness = (torch.mm(Network.negative_labels, Layer.hebbian_weights_layer_two_zeroth) * positive_output + torch.mm(Network.negative_labels, Layer.hebbian_weights_layer_two) * negative_output.pow(2) + torch.mm(Network.negative_labels, Layer.hebbian_weights_layer_two_quadra) * negative_output.pow(4) + torch.mm(Network.negative_labels, Layer.hebbian_weights_layer_two_third) * negative_output.pow(3)).mean(1)\n",
    "                negative_goodness = (torch.mm(Network.negative_labels, Layer.hebbian_weights_layer_two) * negative_output.pow(2)).mean(1)\n",
    "            \n",
    "            positive_goodness_history.append(positive_goodness.mean().item())\n",
    "            negative_goodness_history.append(negative_goodness.mean().item())\n",
    "            difference_history.append(positive_goodness.mean().item() - negative_goodness.mean().item())\n",
    "\n",
    "            latest_difference = positive_goodness - negative_goodness\n",
    "            \n",
    "            # Network.layer_differences = Network.layer_differences.clone()\n",
    "            Network.layer_differences[:, layer_num] = latest_difference.detach().cuda()\n",
    "\n",
    "\n",
    "            if self.is_hinge_loss:\n",
    "                loss = self.exponential_hinge_loss(positive_goodness, negative_goodness)\n",
    "            else:\n",
    "                loss = self.balanced_loss(positive_goodness, negative_goodness, difference_sum)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            self.hebbian_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.hebbian_optimizer.step()\n",
    "\n",
    "        # self.plot_goodness(positive_goodness_history, negative_goodness_history, difference_history)\n",
    "        return self.forward(positive_input).detach(), self.forward(negative_input).detach()\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "    # hebbian_weights = nn.Parameter(torch.ones(10, 774).cuda())\n",
    "    positive_labels = []\n",
    "    negative_labels= []\n",
    "    layer_differences = torch.zeros(60000, 2).cuda()\n",
    "    def __init__(self, dimension_configs):\n",
    "        super().__init__()\n",
    "        self.layers = []\n",
    "        for i in range(len(dimension_configs) - 1):\n",
    "            self.layers += [Layer(dimension_configs[i], dimension_configs[i + 1]).cuda()]\n",
    "            \n",
    "            \n",
    "    def balanced_loss(self, positive_goodness, negative_goodness, alpha=4.0):\n",
    "        delta = positive_goodness - negative_goodness\n",
    "        per_instance_loss = torch.log(1 + torch.exp(-alpha * delta))\n",
    "        return per_instance_loss.mean()  \n",
    "\n",
    "\n",
    "    def mark_data(self, data, label):\n",
    "        marked_data = data.clone().cuda()\n",
    "        return marked_data\n",
    "    \n",
    "    \n",
    "    def predict(self, input_data):\n",
    "        goodness_layer_one = []\n",
    "        goodness_layer_two = []\n",
    "\n",
    "        for layer_num, layer in enumerate(self.layers):\n",
    "            input_data = layer(input_data)\n",
    "            activity = input_data\n",
    "            \n",
    "            for label in range(10):\n",
    "                hebbian_weight_one = Layer.hebbian_weights_layer_one[label, :]\n",
    "                hebbian_weight_two = Layer.hebbian_weights_layer_two[label, :]\n",
    "                if layer_num == 0:\n",
    "                    goodness_value = (activity * hebbian_weight_one).mean(1)\n",
    "                    goodness_layer_one.append(goodness_value.unsqueeze(0))\n",
    "                else:\n",
    "                    goodness_value = (activity * hebbian_weight_two).mean(1)\n",
    "                    goodness_layer_two.append(goodness_value.unsqueeze(0))\n",
    "\n",
    "        # Convert lists to tensors\n",
    "        goodness_layer_one = torch.cat(goodness_layer_one, dim=0)\n",
    "        goodness_layer_two = torch.cat(goodness_layer_two, dim=0)\n",
    "\n",
    "        # Sum the goodness values across all labels\n",
    "        goodness = goodness_layer_one.add_(goodness_layer_two)\n",
    "        # Get the index of the maximum goodness value\n",
    "        return goodness.argmax(dim=0)\n",
    "        \n",
    "    def slow_predict(self, input_data):\n",
    "        goodness_per_label = []\n",
    "        for label in range(10):\n",
    "            marked_data = self.mark_data(input_data, label)\n",
    "            # hebbian_weight_one_zeroth = Layer.hebbian_weights_layer_one_zeroth[label, :]\n",
    "            # hebbian_weight_two_zeroth = Layer.hebbian_weights_layer_two_zeroth[label, :]\n",
    "            hebbian_weight_one = Layer.hebbian_weights_layer_one[label, :]\n",
    "            hebbian_weight_two = Layer.hebbian_weights_layer_two[label, :]\n",
    "            # hebbian_weight_one_third = Layer.hebbian_weights_layer_one_third[label, :]\n",
    "            # hebbian_weight_two_third = Layer.hebbian_weights_layer_two_third[label, :]\n",
    "            # hebbian_weight_one_quadra = Layer.hebbian_weights_layer_one_quadra[label, :]\n",
    "            # hebbian_weight_two_quadra = Layer.hebbian_weights_layer_two_quadra[label, :]\n",
    "            goodness = []\n",
    "            for layer_num, layer in enumerate(self.layers):\n",
    "                marked_data = layer(marked_data)\n",
    "                if layer_num == 0:\n",
    "                    goodness_value = (marked_data.pow(2) * hebbian_weight_one).mean(1)\n",
    "                else:\n",
    "                    goodness_value = (marked_data.pow(2) * hebbian_weight_two).mean(1)\n",
    "                goodness.append(goodness_value)\n",
    "\n",
    "            goodness_per_label.append(torch.sum(torch.stack(goodness), dim=0).unsqueeze(1))\n",
    "        goodness_per_label = torch.cat(goodness_per_label, 1)\n",
    "        return goodness_per_label.argmax(dim=1)\n",
    "    def compute_hebbian_activity(self, values):\n",
    "        labels = values[:, :10].cuda()\n",
    "        hebbian = values[:, 10:].cuda()\n",
    "        hebbian_value = torch.mm(hebbian, self.hebbian_weights.T) * labels\n",
    "        return hebbian_value.mean(1).cuda()\n",
    "    \n",
    "    \n",
    "    def train_network(self, positive_goodness, negative_goodness, training_data, training_data_label, positive_one_hot_labels, negative_one_hot_labels):\n",
    "        \n",
    "        # positive_mean_values = []\n",
    "        # negative_mean_values = []\n",
    "        # \n",
    "        # for _ in tqdm(range(number_of_epochs)):\n",
    "        #     positive_labels = positive_goodness[:, :10].cuda()\n",
    "        #     positive_hebbian = positive_goodness[:, 10:].cuda()\n",
    "        #     positive_hebbian_value = torch.mm(positive_hebbian, self.hebbian_weights.T.pow(2)) * positive_labels\n",
    "        #     positive_mean_value = positive_hebbian_value.mean(1).cuda()\n",
    "        #     positive_mean_values.append(positive_mean_value.mean().item())  # Store average for plotting\n",
    "        # \n",
    "        #     negative_labels = negative_goodness[:, :10].cuda()\n",
    "        #     negative_hebbian = negative_goodness[:, 10:].cuda()\n",
    "        #     negative_hebbian_value = torch.mm(negative_hebbian, self.hebbian_weights.T.pow(2)) * negative_labels\n",
    "        #     negative_mean_value = negative_hebbian_value.mean(1).cuda()\n",
    "        #     negative_mean_values.append(negative_mean_value.mean().item())  # Store average for plotting\n",
    "        # \n",
    "        #     loss = self.balanced_loss(positive_mean_value, negative_mean_value)\n",
    "        #     self.hebbian_optimizer.zero_grad()\n",
    "        #     loss.backward()\n",
    "        #     self.hebbian_optimizer.step()\n",
    "        # \n",
    "        # plt.figure(figsize=(10, 5))\n",
    "        # plt.plot(positive_mean_values, label='Positive Mean Value', color='blue')\n",
    "        # plt.plot(negative_mean_values, label='Negative Mean Value', color='red')\n",
    "        # plt.title('Change in Mean Values Over Epochs')\n",
    "        # plt.xlabel('Epochs')\n",
    "        # plt.ylabel('Mean Value')\n",
    "        # plt.legend()\n",
    "        # plt.grid(True)\n",
    "        # plt.show()\n",
    "        # \n",
    "        for epoch in tqdm(range(1)):\n",
    "            goodness_pos, goodness_neg = positive_goodness, negative_goodness\n",
    "            negative_goodness, negative_one_hot_labels = create_negative_data(training_data, training_data_label)\n",
    "            goodness_neg = negative_goodness\n",
    "            positive_labels = goodness_pos[:, :10]\n",
    "            negative_labels = negative_goodness[:, :10]\n",
    "\n",
    "            Network.positive_labels = positive_one_hot_labels\n",
    "            Network.negative_labels = negative_one_hot_labels\n",
    "            for i, layer in enumerate(self.layers):\n",
    "                print('Training Layer', i, '...')\n",
    "                goodness_pos, goodness_neg = layer.train_layer(goodness_pos, goodness_neg, i)\n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "def load_CIFAR10_data(train_batch_size=30000, test_batch_size=6000):\n",
    "    data_transformation = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)), \n",
    "        Lambda(lambda x: torch.flatten(x))\n",
    "    ])\n",
    "\n",
    "    training_data_loader = DataLoader(\n",
    "        CIFAR10('./data/', train=True, download=True, transform=data_transformation),\n",
    "        batch_size=train_batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    testing_data_loader = DataLoader(\n",
    "        CIFAR10('./data/', train=False, download=True, transform=data_transformation),\n",
    "        batch_size=test_batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    return training_data_loader, testing_data_loader\n",
    "\n",
    "def load_FashionMNIST_data(train_batch_size=60000, test_batch_size=10000):\n",
    "    data_transformation = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize((0.2860,), (0.3530,)),\n",
    "        Lambda(lambda x: torch.flatten(x))\n",
    "    ])\n",
    "\n",
    "    training_data_loader = DataLoader(\n",
    "        FashionMNIST('./data/', train=True, download=True, transform=data_transformation),\n",
    "        batch_size=train_batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    testing_data_loader = DataLoader(\n",
    "        FashionMNIST('./data/', train=False, download=True, transform=data_transformation),\n",
    "        batch_size=test_batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    return training_data_loader, testing_data_loader\n",
    "\n",
    "def load_MNIST_data(train_batch_size=50000, test_batch_size=10000):\n",
    "    data_transformation = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize((0.1307,), (0.3081,)),\n",
    "        Lambda(lambda x: torch.flatten(x))\n",
    "    ])\n",
    "\n",
    "    training_data_loader = DataLoader(\n",
    "        MNIST('./data/', train=True, download=True, transform=data_transformation),\n",
    "        batch_size=train_batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    testing_data_loader = DataLoader(\n",
    "        MNIST('./data/', train=False, download=True, transform=data_transformation),\n",
    "        batch_size=test_batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    return training_data_loader, testing_data_loader\n",
    "\n",
    "\n",
    "def create_positive_data(data, label):\n",
    "    positive_data = data.clone()\n",
    "\n",
    "    return positive_data\n",
    "\n",
    "\n",
    "def create_negative_data(data, label, num_classes=10, seed=1234):\n",
    "    \"\"\"\n",
    "    This function modifies the labels to create negative samples and returns both the data and one-hot encoded negative labels.\n",
    "    \n",
    "    Args:\n",
    "    - data (torch.Tensor): Tensor of data samples.\n",
    "    - label (torch.Tensor): Original labels of the data.\n",
    "    - num_classes (int): Number of classes in the dataset.\n",
    "    - seed (int, optional): Seed for random number generator.\n",
    "    \n",
    "    Returns:\n",
    "    - torch.Tensor: Original data (unchanged here, but could be modified if necessary).\n",
    "    - torch.Tensor: One-hot encoded negative labels.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "\n",
    "    negative_data = data.clone()\n",
    "    negative_labels = torch.zeros(data.size(0), num_classes, device=data.device)\n",
    "\n",
    "    for i in range(negative_data.shape[0]):\n",
    "        possible_answers = list(range(num_classes))\n",
    "        possible_answers.remove(label[i].item())\n",
    "        false_label = random.choice(possible_answers)\n",
    "        negative_labels[i, false_label] = 1\n",
    "\n",
    "    return negative_data, negative_labels\n",
    "\n",
    "def create_one_hot_labels(labels, num_classes=10):\n",
    "    \"\"\"\n",
    "    This function takes a batch of labels and the number of classes and\n",
    "    returns a one-hot encoded tensor of the labels.\n",
    "\n",
    "    Args:\n",
    "    - labels (torch.Tensor): A tensor of labels of size (N, )\n",
    "    - num_classes (int): The number of classes in the dataset\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: A one-hot encoded tensor of size (N, num_classes)\n",
    "    \"\"\"\n",
    "    # Create a tensor of zeros with size [len(labels), num_classes]\n",
    "    one_hot_labels = torch.zeros(len(labels), num_classes, device=labels.device)\n",
    "    \n",
    "    # Use scatter_ to assign 1s based on label indices\n",
    "    one_hot_labels.scatter_(1, labels.unsqueeze(1), 1)\n",
    "    \n",
    "    return one_hot_labels\n",
    "\n",
    "def prepare_data():\n",
    "    torch.manual_seed(4321)\n",
    "    training_data_loader, testing_data_loader = load_FashionMNIST_data()\n",
    "\n",
    "    training_data, training_data_label = next(iter(training_data_loader))\n",
    "\n",
    "    testing_data, testing_data_label = next(iter(testing_data_loader))\n",
    "    testing_data, testing_data_label = testing_data.cuda(), testing_data_label.cuda()\n",
    "\n",
    "    print(f\"Training Data: \", training_data)\n",
    "    print(f\"Training Data Label: \", training_data_label)\n",
    "\n",
    "    training_data, training_data_label = training_data.cuda(), training_data_label.cuda()\n",
    "    \n",
    "    positive_data = create_positive_data(training_data, training_data_label)\n",
    "    print(f\"Positive Data: \", positive_data)\n",
    "\n",
    "    negative_data, negative_one_hot_labels = create_negative_data(training_data.cuda(), training_data_label.cuda())\n",
    "    print(f\"Negative Data: \", negative_data)\n",
    "\n",
    "    return positive_data, negative_data, negative_one_hot_labels, training_data, training_data_label, testing_data, testing_data_label, training_data_loader\n",
    "\n",
    "def measure_execution_time_with_uncertainty(model, input_data, repetitions=100):\n",
    "    times = []  # List to store execution times for each repetition\n",
    "\n",
    "    for _ in range(repetitions):\n",
    "        start_time = time.perf_counter()  # Start timing before prediction\n",
    "        _ = model.predict(input_data)  # Execute the predict function\n",
    "        end_time = time.perf_counter()  # End timing after prediction\n",
    "        times.append(end_time - start_time)  # Append the time difference to the list\n",
    "\n",
    "    times = np.array(times)  # Convert list to NumPy array for easier calculations\n",
    "    average_time = np.mean(times)  # Calculate the average of the execution times\n",
    "    std_deviation = np.std(times)  # Calculate the standard deviation of the execution times\n",
    "\n",
    "    return average_time, std_deviation\n",
    "\n",
    "\n",
    "def measure_slow_execution_time_with_uncertainty(model, input_data, repetitions=100):\n",
    "    times = []  # List to store execution times for each repetition\n",
    "\n",
    "    for _ in range(repetitions):\n",
    "        start_time = time.perf_counter()  # Start timing before prediction\n",
    "        _ = model.slow_predict(input_data)  # Execute the predict function\n",
    "        end_time = time.perf_counter()  # End timing after prediction\n",
    "        times.append(end_time - start_time)  # Append the time difference to the list\n",
    "\n",
    "    times = np.array(times)  # Convert list to NumPy array for easier calculations\n",
    "    average_time = np.mean(times)  # Calculate the average of the execution times\n",
    "    std_deviation = np.std(times)  # Calculate the standard deviation of the execution times\n",
    "\n",
    "    return average_time, std_deviation\n",
    "\n",
    "def measure_single_execution_time_with_uncertainty(model, input_data, repetitions=100):\n",
    "    model.eval()  # Ensure the model is in evaluation mode\n",
    "    times = []  # List to store execution times for each repetition\n",
    "\n",
    "    for _ in range(repetitions):\n",
    "        # Ensure a single instance is loaded to GPU\n",
    "        single_data = input_data[0:1].cuda()  # Taking the first instance for measurement\n",
    "        torch.cuda.synchronize()  # Wait for all prior operations to complete\n",
    "\n",
    "        start_time = time.perf_counter()  # Start timing\n",
    "        with torch.no_grad():  # Context manager to turn off gradient computation\n",
    "            _ = model.predict(single_data)  # Execute the predict function\n",
    "        torch.cuda.synchronize()  # Ensure completion of CUDA operations\n",
    "\n",
    "        end_time = time.perf_counter()  # End timing\n",
    "        times.append(end_time - start_time)  # Append the time difference to the list\n",
    "\n",
    "    times = np.array(times)  # Convert list to NumPy array for easier calculations\n",
    "    average_time = np.mean(times)  # Calculate the average of the execution times\n",
    "    std_deviation = np.std(times)  # Calculate the standard deviation of the execution times\n",
    "\n",
    "    return average_time, std_deviation\n",
    "\n",
    "\n",
    "def measure_single_slow_execution_time_with_uncertainty(model, input_data, repetitions=100):\n",
    "    model.eval()  # Ensure the model is in evaluation mode\n",
    "    times = []  # List to store execution times for each repetition\n",
    "\n",
    "    for _ in range(repetitions):\n",
    "        # Ensure a single instance is loaded to GPU\n",
    "        single_data = input_data[0:1].cuda()  # Taking the first instance for measurement\n",
    "        torch.cuda.synchronize()  # Wait for all prior operations to complete\n",
    "\n",
    "        start_time = time.perf_counter()  # Start timing\n",
    "        with torch.no_grad():  # Context manager to turn off gradient computation\n",
    "            _ = model.slow_predict(single_data)  # Execute the predict function\n",
    "        torch.cuda.synchronize()  # Ensure completion of CUDA operations\n",
    "\n",
    "        end_time = time.perf_counter()  # End timing\n",
    "        times.append(end_time - start_time)  # Append the time difference to the list\n",
    "\n",
    "    times = np.array(times)  # Convert list to NumPy array for easier calculations\n",
    "    average_time = np.mean(times)  # Calculate the average of the execution times\n",
    "    std_deviation = np.std(times)  # Calculate the standard deviation of the execution times\n",
    "\n",
    "    return average_time, std_deviation\n",
    "\n",
    "\n",
    "def measure_individual_throughput(model, data_loader, repetitions=3):\n",
    "    model.eval()  # Ensure the model is in evaluation mode\n",
    "    throughputs = []\n",
    "\n",
    "    for _ in range(repetitions):\n",
    "        total_images = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for image, _ in data_loader:  # image is now a single instance, not a batch\n",
    "            image = image.cuda(non_blocking=True)  # Transfer data to GPU if available\n",
    "            with torch.no_grad():  # No need to compute gradients\n",
    "                _ = model.predict(image)  # Perform predictions\n",
    "\n",
    "            total_images += 1  # Increment image count by one\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        throughput = total_images / elapsed_time  # Calculate throughput as images per second\n",
    "        throughputs.append(throughput)\n",
    "\n",
    "    # Compute average and standard deviation of throughput\n",
    "    average_throughput = np.mean(throughputs)\n",
    "    std_dev_throughput = np.std(throughputs)\n",
    "    \n",
    "    return average_throughput, std_dev_throughput\n",
    "\n",
    "\n",
    "\n",
    "def measure_slow_individual_throughput(model, data_loader, repetitions=3):\n",
    "    model.eval()  # Ensure the model is in evaluation mode\n",
    "    throughputs = []\n",
    "\n",
    "    for _ in range(repetitions):\n",
    "        total_images = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for image, _ in data_loader:  # image is now a single instance, not a batch\n",
    "            image = image.cuda(non_blocking=True)  # Transfer data to GPU if available\n",
    "            with torch.no_grad():  # No need to compute gradients\n",
    "                _ = model.slow_predict(image)  # Perform predictions\n",
    "\n",
    "            total_images += 1  # Increment image count by one\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        throughput = total_images / elapsed_time  # Calculate throughput as images per second\n",
    "        throughputs.append(throughput)\n",
    "\n",
    "    # Compute average and standard deviation of throughput\n",
    "    average_throughput = np.mean(throughputs)\n",
    "    std_dev_throughput = np.std(throughputs)\n",
    "    \n",
    "    return average_throughput, std_dev_throughput\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.manual_seed(1234)\n",
    "    positive_data, negative_data, negative_one_hot_labels, training_data, training_data_label, testing_data, testing_data_label, training_data_loader = prepare_data()\n",
    "    positive_one_hot_labels = create_one_hot_labels(training_data_label)\n",
    "    network = Network([784, hidden_layer_neurons, hidden_layer_neurons]).cuda() #3072\n",
    "    network.train_network(positive_data, negative_data, training_data, training_data_label, positive_one_hot_labels, negative_one_hot_labels)\n",
    "    del Network.layer_differences\n",
    "    del positive_data\n",
    "    del negative_data\n",
    "    del Network.positive_labels\n",
    "    del Network.negative_labels\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Training Accuracy: \", network.predict(training_data).eq(training_data_label).float().mean().item())\n",
    "    print(\"Testing Accuracy: \", network.predict(testing_data).eq(testing_data_label).float().mean().item())\n",
    "    average_time, uncertainty = measure_execution_time_with_uncertainty(network, testing_data)\n",
    "    print(f\"Average Execution Time: {average_time:.6f} seconds ± {uncertainty:.6f} seconds\")\n",
    "    average_time, uncertainty = measure_slow_execution_time_with_uncertainty(network, testing_data)\n",
    "    print(f\"SLOW_Average Execution Time: {average_time:.6f} seconds ± {uncertainty:.6f} seconds\")\n",
    "    single_average_time, single_uncertainty = measure_single_execution_time_with_uncertainty(network, testing_data)\n",
    "    print(f\"Single Data Execution Time: {single_average_time:.6f} seconds ± {single_uncertainty:.6f} seconds\")\n",
    "    single_average_time, single_uncertainty = measure_single_slow_execution_time_with_uncertainty(network, testing_data)\n",
    "    print(f\"SLOW_Single Data Execution Time: {single_average_time:.6f} seconds ± {single_uncertainty:.6f} seconds\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e49f6534f3c76302"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
