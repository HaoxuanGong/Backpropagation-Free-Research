{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-04T08:30:24.337956600Z",
     "start_time": "2024-09-04T08:30:15.427620900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from matplotlib) (4.50.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from matplotlib) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from matplotlib) (6.3.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.18.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Training Data:  tensor([[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "        [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "        [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "        ...,\n",
      "        [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "        [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "        [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]])\n",
      "Training Data Label:  tensor([5, 0, 4,  ..., 1, 6, 2])\n",
      "Positive Data:  tensor([[ 0.0000,  0.0000,  0.0000,  ..., -0.4242, -0.4242, -0.4242],\n",
      "        [ 1.0000,  0.0000,  0.0000,  ..., -0.4242, -0.4242, -0.4242],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ..., -0.4242, -0.4242, -0.4242],\n",
      "        ...,\n",
      "        [ 0.0000,  1.0000,  0.0000,  ..., -0.4242, -0.4242, -0.4242],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ..., -0.4242, -0.4242, -0.4242],\n",
      "        [ 0.0000,  0.0000,  1.0000,  ..., -0.4242, -0.4242, -0.4242]],\n",
      "       device='cuda:0')\n",
      "Negative Data:  tensor([[ 0.0000,  0.0000,  0.0000,  ..., -0.4242, -0.4242, -0.4242],\n",
      "        [ 0.0000,  0.0000,  1.0000,  ..., -0.4242, -0.4242, -0.4242],\n",
      "        [ 1.0000,  0.0000,  0.0000,  ..., -0.4242, -0.4242, -0.4242],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ..., -0.4242, -0.4242, -0.4242],\n",
      "        [ 0.0000,  1.0000,  0.0000,  ..., -0.4242, -0.4242, -0.4242],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ..., -0.4242, -0.4242, -0.4242]],\n",
      "       device='cuda:0')\n",
      "Training Layer 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 248.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 167.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 325.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 200.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 329.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 199.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 342.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 249.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 250.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 249.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 332.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 199.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 333.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 250.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 224.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 82.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 200.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 71.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 202.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 71.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 166.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 67.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 166.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 62.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 144.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 58.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 142.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 62.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 142.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 53.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 142.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 52.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 141.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 48.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 143.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 50.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 142.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 45.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 142.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 44.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 125.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 43.47it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 360\u001B[0m\n\u001B[0;32m    358\u001B[0m positive_data, negative_data, training_data, training_data_label, testing_data, testing_data_label, training_data_loader, testing_data_loader \u001B[38;5;241m=\u001B[39m prepare_data()\n\u001B[0;32m    359\u001B[0m network \u001B[38;5;241m=\u001B[39m Network([\u001B[38;5;241m784\u001B[39m, hidden_neuron_number, hidden_neuron_number])\u001B[38;5;241m.\u001B[39mcuda() \u001B[38;5;66;03m#3072\u001B[39;00m\n\u001B[1;32m--> 360\u001B[0m \u001B[43mnetwork\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_network\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtraining_data_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtesting_data_loader\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    362\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTraining Accuracy: \u001B[39m\u001B[38;5;124m\"\u001B[39m, network\u001B[38;5;241m.\u001B[39mpredict(training_data)\u001B[38;5;241m.\u001B[39meq(training_data_label)\u001B[38;5;241m.\u001B[39mfloat()\u001B[38;5;241m.\u001B[39mmean()\u001B[38;5;241m.\u001B[39mitem())\n\u001B[0;32m    364\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTesting Accuracy: \u001B[39m\u001B[38;5;124m\"\u001B[39m, network\u001B[38;5;241m.\u001B[39mpredict(testing_data)\u001B[38;5;241m.\u001B[39meq(testing_data_label)\u001B[38;5;241m.\u001B[39mfloat()\u001B[38;5;241m.\u001B[39mmean()\u001B[38;5;241m.\u001B[39mitem())\n",
      "Cell \u001B[1;32mIn[3], line 224\u001B[0m, in \u001B[0;36mNetwork.train_network\u001B[1;34m(self, training_data_loader, testing_data_loader)\u001B[0m\n\u001B[0;32m    222\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m images, labels \u001B[38;5;129;01min\u001B[39;00m training_data_loader:\n\u001B[0;32m    223\u001B[0m     positive_data \u001B[38;5;241m=\u001B[39m create_positive_data(images, labels)\u001B[38;5;241m.\u001B[39mcuda()\n\u001B[1;32m--> 224\u001B[0m     negative_data \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_negative_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mcuda()\n\u001B[0;32m    225\u001B[0m     goodness_pos, goodness_neg \u001B[38;5;241m=\u001B[39m positive_data, negative_data\n\u001B[0;32m    226\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers):\n",
      "Cell \u001B[1;32mIn[3], line 325\u001B[0m, in \u001B[0;36mcreate_negative_data\u001B[1;34m(data, label, seed)\u001B[0m\n\u001B[0;32m    323\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(negative_data\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]):\n\u001B[0;32m    324\u001B[0m     possible_answers \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m10\u001B[39m))\n\u001B[1;32m--> 325\u001B[0m     \u001B[43mpossible_answers\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mremove\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabel\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    326\u001B[0m     false_label \u001B[38;5;241m=\u001B[39m random\u001B[38;5;241m.\u001B[39mchoice(possible_answers)\n\u001B[0;32m    327\u001B[0m     negative_data[i][false_label] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1.0\u001B[39m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n",
    "!pip install matplotlib\n",
    "\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST, FashionMNIST, CIFAR10\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Lambda\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch import Tensor\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import Adam\n",
    "\n",
    "number_of_epochs = 200\n",
    "\n",
    "hidden_neuron_number = 2000\n",
    "\n",
    "def regularize_matrix(A, lambda_val=1e-5):\n",
    "    n = A.size(0)  # Assuming A is square\n",
    "    I = torch.eye(n, dtype=A.dtype, device=A.device)\n",
    "    A_regularized = A + lambda_val * I\n",
    "    return A_regularized\n",
    "\n",
    "def add_noise(A, epsilon=1e-5):\n",
    "    noise = (torch.randn(A.shape) * epsilon).cuda()\n",
    "    return A + noise\n",
    "\n",
    "def find_particular_solution(A, b):\n",
    "    A_pinv = torch.linalg.pinv(A)  # Compute pseudoinverse\n",
    "    x_p = A_pinv @ b\n",
    "    return x_p\n",
    "\n",
    "class Layer(nn.Linear):\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True, device=None, d_type=None, is_hinge_loss=True):\n",
    "        super().__init__(in_features, out_features, bias, device, d_type)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.learning_rate = 0.02\n",
    "        self.optimizer = Adam(self.parameters(), lr=self.learning_rate)\n",
    "        # self.layer_weights = nn.Parameter(torch.ones(6, 800))\n",
    "        self.threshold = 2.0\n",
    "        self.num_of_epochs = number_of_epochs\n",
    "        self.is_hinge_loss = is_hinge_loss\n",
    "        \n",
    "        \n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        normalized_input = input / (input.norm(2, 1, keepdim=True) + 1e-4)\n",
    "        output = torch.mm(normalized_input, self.weight.T) + self.bias.unsqueeze(0)\n",
    "        return self.activation(output)\n",
    "\n",
    "\n",
    "    def balanced_loss(self, positive_goodness, negative_goodness, alpha=4.0):\n",
    "        delta = positive_goodness - negative_goodness\n",
    "        per_instance_loss = torch.log(1 + torch.exp(-alpha * delta))\n",
    "        return per_instance_loss.mean()  # Averaging the loss to ensure it's a scalar\n",
    "\n",
    "\n",
    "    def soft_plus_loss(self, positive_goodness, negative_goodness, is_second_phase=False):\n",
    "        if is_second_phase:\n",
    "            threshold = self.threshold * 2\n",
    "        else:\n",
    "            threshold = self.threshold\n",
    "        return torch.log(1 + torch.exp(torch.cat([\n",
    "            -positive_goodness + threshold,\n",
    "            negative_goodness - threshold]))).mean()\n",
    "\n",
    "\n",
    "\n",
    "    def plot_goodness(self, positive_goodness_history, negative_goodness_history,\n",
    "                      positive_unaltered_goodness_history, negative_unaltered_goodness_history):\n",
    "        epochs = range(1, self.num_of_epochs + 1)\n",
    "\n",
    "        plt.figure(figsize=(14, 7))\n",
    "\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(epochs, positive_goodness_history, label='Altered Positive Goodness')\n",
    "        plt.plot(epochs, positive_unaltered_goodness_history, label='Unaltered Positive Goodness')\n",
    "        plt.legend()\n",
    "        plt.title('Positive Goodness Comparison Over Epochs')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Goodness')\n",
    "\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(epochs, negative_goodness_history, label='Altered Negative Goodness')\n",
    "        plt.plot(epochs, negative_unaltered_goodness_history, label='Unaltered Negative Goodness')\n",
    "        plt.legend()\n",
    "        plt.title('Negative Goodness Comparison Over Epochs')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Goodness')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_goodness(self, positive_goodness, negative_goodness, difference):\n",
    "        plt.figure(figsize=(10, 5), dpi=400)  # Higher dpi for better resolution\n",
    "        plt.plot(positive_goodness, label='Positive Goodness', color='b')\n",
    "        plt.plot(negative_goodness, label='Negative Goodness', color='r')\n",
    "        plt.plot(difference, label='Difference Goodness', color='g')\n",
    "        plt.xlabel('Epoch', fontsize=14)\n",
    "        plt.ylabel('Goodness Value', fontsize=14)\n",
    "        plt.title('Change in Goodness During Training', fontsize=16)\n",
    "        plt.legend(fontsize=12)\n",
    "        plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "        plt.tick_params(labelsize=12)\n",
    "        plt.tight_layout()  # Adjusts plot to ensure everything fits without overlap\n",
    "        plt.savefig('Goodness_Tracking.pdf', format='pdf')  # Save as PDF\n",
    "        plt.show()\n",
    "\n",
    "    def plot_difference(self, difference):\n",
    "        plt.figure(figsize=(10, 5), dpi=400)\n",
    "        plt.plot(difference, label='Difference Goodness', color='g')\n",
    "        plt.xlabel('Epoch', fontsize=14)\n",
    "        plt.ylabel('Goodness Value', fontsize=14)\n",
    "        plt.title('Change in Goodness During Training', fontsize=16)\n",
    "        plt.legend(fontsize=12)\n",
    "        plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "        plt.tick_params(labelsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('Difference_Goodness.pdf', format='pdf')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def train_layer(self, positive_input, negative_input, layer_num):\n",
    "        positive_goodness_history = []\n",
    "        negative_goodness_history = []\n",
    "        difference_history = []\n",
    "        positive_output = self.forward(positive_input)  # Shape: [batch_size, 500]\n",
    "        negative_output = self.forward(negative_input)\n",
    "    \n",
    "        positive_goodness = (positive_output.pow(2)).mean(1)  # Shape: [batch_size]\n",
    "        negative_goodness = (negative_output.pow(2)).mean(1)\n",
    "            \n",
    "        positive_goodness_history.append(positive_goodness.mean().item())\n",
    "        negative_goodness_history.append(negative_goodness.mean().item())\n",
    "        difference_history.append(positive_goodness.mean().item() - negative_goodness.mean().item())\n",
    "        if self.is_hinge_loss:\n",
    "            loss = self.soft_plus_loss(positive_goodness, negative_goodness)\n",
    "        else:\n",
    "            loss = self.balanced_loss(positive_goodness, negative_goodness)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "            \n",
    "        # layer_weight_row = Layer.layer_weights[layer_num, :]\n",
    "        # layer_weight_four_row = Layer.layer_weights_four[layer_num, :]\n",
    "        # positive_output = self.forward(positive_input)\n",
    "        # negative_output = self.forward(negative_input)\n",
    "        \n",
    "        # if layer_num == 0:\n",
    "        #     # Layer.layer_values_one = torch.cat((positive_output.pow(2) * layer_weight_row + positive_output.pow(4) * layer_weight_four_row, negative_output.pow(2) * layer_weight_row + negative_output.pow(4) * layer_weight_four_row), dim=0)\n",
    "        #     # \n",
    "        #     new_values_one = (positive_output.pow(2) * layer_weight_row + positive_output.pow(4) * layer_weight_four_row).mean(1).detach().clone()\n",
    "        #     print(f\"SHAPE: {new_values_one.shape}\")\n",
    "        #     new_values_two = (negative_output.pow(2) * layer_weight_row + negative_output.pow(4) * layer_weight_four_row).mean(1).detach().clone()\n",
    "        #     Layer.layer_one_sum = torch.cat((new_values_one, new_values_two), dim=0)\n",
    "        #     \n",
    "        # \n",
    "        #     \n",
    "        # else:\n",
    "        #     # Layer.layer_values_two = torch.cat((positive_output.pow(2) * layer_weight_row + positive_output.pow(4) * layer_weight_four_row, negative_output.pow(2) * layer_weight_row + negative_output.pow(4) * layer_weight_four_row), dim=0)\n",
    "        #     \n",
    "        #     new_values_one = (positive_output.pow(2) * layer_weight_row + positive_output.pow(4) * layer_weight_four_row).mean(1).detach().clone()\n",
    "        #     new_values_two = (negative_output.pow(2) * layer_weight_row + negative_output.pow(4) * layer_weight_four_row).mean(1).detach().clone()\n",
    "        #     Layer.layer_two_sum = torch.cat((new_values_one, new_values_two), dim=0)\n",
    "        #     \n",
    "        # self.plot_goodness(positive_goodness_history, negative_goodness_history, difference_history)\n",
    "        # self.plot_difference(difference_history)\n",
    "        return self.forward(positive_input).detach(), self.forward(negative_input).detach()\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self, dimension_configs):\n",
    "        super().__init__()\n",
    "        self.layers = []\n",
    "        self.softmax_weights = nn.Parameter(torch.randn(hidden_neuron_number*(len(dimension_configs) - 1), 10).cuda())\n",
    "        self.optimizer = Adam([self.softmax_weights], lr=0.001)\n",
    "        #self.layer_weights = nn.Parameter(torch.ones(2))\n",
    "        for i in range(len(dimension_configs) - 1):\n",
    "            self.layers += [Layer(dimension_configs[i], dimension_configs[i + 1]).cuda()]\n",
    "\n",
    "    def mark_data(self, data, label):\n",
    "        marked_data = data.clone().cuda()\n",
    "        marked_data[:, :10] = 0\n",
    "        marked_data[torch.arange(marked_data.size(0)), label] = 1\n",
    "        return marked_data\n",
    "# Layer.layer_final_weights[layer_num]\n",
    "    def predict(self, input_data):\n",
    "        input_data[:, :10] = 0.1  # Modifying the first 10 data entries\n",
    "        layer_one_output = None\n",
    "        layer_two_output = None\n",
    "\n",
    "        # Process input data through each layer and capture specific layer outputs\n",
    "        for layer_num, layer in enumerate(self.layers):\n",
    "            input_data = layer(input_data)\n",
    "            if layer_num == 0:\n",
    "                layer_one_output = input_data.clone()  # Use clone() to avoid in-place modifications affecting this tensor\n",
    "            elif layer_num == 1:\n",
    "                layer_two_output = input_data.clone()  # Similarly, clone to ensure the tensor is not modified later\n",
    "\n",
    "        # Ensure both layer outputs are captured and not None\n",
    "        if layer_one_output is None or layer_two_output is None:\n",
    "            raise ValueError(\"Layer outputs are not properly captured, check the number of layers and indices.\")\n",
    "\n",
    "        # Concatenate outputs from the two specified layers\n",
    "        all_inputs = torch.cat((layer_one_output, layer_two_output), dim=1)\n",
    "\n",
    "        # Compute logits using softmax weights\n",
    "        logits = torch.mm(all_inputs, self.softmax_weights)\n",
    "        return logits.argmax(dim=1)  # Return the predicted class indices\n",
    "    \n",
    "    \n",
    "    def train_network(self, training_data_loader, testing_data_loader):\n",
    "        for epoch in tqdm(range(number_of_epochs)):\n",
    "            for images, labels in training_data_loader:\n",
    "                positive_data = create_positive_data(images, labels).cuda()\n",
    "                negative_data = create_negative_data(images, labels).cuda()\n",
    "                goodness_pos, goodness_neg = positive_data, negative_data\n",
    "                for i, layer in enumerate(self.layers):\n",
    "                    print('Training Layer', i, '...')\n",
    "                    goodness_pos, goodness_neg = layer.train_layer(goodness_pos, goodness_neg, i)\n",
    "        \n",
    "                positive_data[:, :10] = 0.1\n",
    "                positive_output_first_layer = self.layers[0].forward(positive_data).detach()\n",
    "                positive_output_second_layer = self.layers[1].forward(positive_output_first_layer).detach()\n",
    "\n",
    "                all_outputs = torch.cat((positive_output_first_layer, positive_output_second_layer), dim=1).detach()\n",
    "                logits = torch.mm(all_outputs, self.softmax_weights)\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "                loss = criterion(logits, training_data_label)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "def load_CIFAR10_data(train_batch_size=30000, test_batch_size=6000):\n",
    "    data_transformation = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)), \n",
    "        Lambda(lambda x: torch.flatten(x))\n",
    "    ])\n",
    "\n",
    "    training_data_loader = DataLoader(\n",
    "        CIFAR10('./data/', train=True, download=True, transform=data_transformation),\n",
    "        batch_size=train_batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    testing_data_loader = DataLoader(\n",
    "        CIFAR10('./data/', train=False, download=True, transform=data_transformation),\n",
    "        batch_size=test_batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    return training_data_loader, testing_data_loader\n",
    "\n",
    "def load_FashionMNIST_data(train_batch_size=60000, test_batch_size=10000):\n",
    "    data_transformation = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize((0.2860,), (0.3530,)),\n",
    "        Lambda(lambda x: torch.flatten(x))\n",
    "    ])\n",
    "\n",
    "    training_data_loader = DataLoader(\n",
    "        FashionMNIST('./data/', train=True, download=True, transform=data_transformation),\n",
    "        batch_size=train_batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    testing_data_loader = DataLoader(\n",
    "        FashionMNIST('./data/', train=False, download=True, transform=data_transformation),\n",
    "        batch_size=test_batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    return training_data_loader, testing_data_loader\n",
    "\n",
    "def load_MNIST_data(train_batch_size=2500, test_batch_size=500):\n",
    "    data_transformation = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize((0.1307,), (0.3081,)),\n",
    "        Lambda(lambda x: torch.flatten(x))\n",
    "    ])\n",
    "\n",
    "    training_data_loader = DataLoader(\n",
    "        MNIST('./data/', train=True, download=True, transform=data_transformation),\n",
    "        batch_size=train_batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    testing_data_loader = DataLoader(\n",
    "        MNIST('./data/', train=False, download=True, transform=data_transformation),\n",
    "        batch_size=test_batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    return training_data_loader, testing_data_loader\n",
    "\n",
    "\n",
    "def create_positive_data(data, label):\n",
    "    positive_data = data.clone()\n",
    "    positive_data[:, :10] = 0.0\n",
    "\n",
    "    for i in range(positive_data.shape[0]):\n",
    "        positive_data[i][label[i]] = 1.0\n",
    "\n",
    "    return positive_data\n",
    "\n",
    "\n",
    "def create_negative_data(data, label, seed=None):\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "\n",
    "    negative_data = data.clone()\n",
    "    negative_data[:, :10] = 0.0\n",
    "\n",
    "    for i in range(negative_data.shape[0]):\n",
    "        possible_answers = list(range(10))\n",
    "        possible_answers.remove(label[i])\n",
    "        false_label = random.choice(possible_answers)\n",
    "        negative_data[i][false_label] = 1.0\n",
    "\n",
    "    return negative_data\n",
    "\n",
    "\n",
    "def prepare_data():\n",
    "    torch.manual_seed(1234)\n",
    "    training_data_loader, testing_data_loader = load_MNIST_data()\n",
    "\n",
    "    training_data, training_data_label = next(iter(training_data_loader))\n",
    "\n",
    "    testing_data, testing_data_label = next(iter(testing_data_loader))\n",
    "    testing_data, testing_data_label = testing_data.cuda(), testing_data_label.cuda()\n",
    "\n",
    "    print(f\"Training Data: \", training_data)\n",
    "    print(f\"Training Data Label: \", training_data_label)\n",
    "\n",
    "    training_data, training_data_label = training_data.cuda(), training_data_label.cuda()\n",
    "\n",
    "    positive_data = create_positive_data(training_data, training_data_label)\n",
    "    print(f\"Positive Data: \", positive_data)\n",
    "\n",
    "    negative_data = create_negative_data(training_data, training_data_label, seed=1234)\n",
    "    print(f\"Negative Data: \", negative_data)\n",
    "\n",
    "    return positive_data, negative_data, training_data, training_data_label, testing_data, testing_data_label, training_data_loader, testing_data_loader\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.manual_seed(1234)\n",
    "    positive_data, negative_data, training_data, training_data_label, testing_data, testing_data_label, training_data_loader,testing_data_loader = prepare_data()\n",
    "    network = Network([784, hidden_neuron_number, hidden_neuron_number]).cuda() #3072\n",
    "    network.train_network(training_data_loader, testing_data_loader)\n",
    "\n",
    "    print(\"Training Accuracy: \", network.predict(training_data).eq(training_data_label).float().mean().item())\n",
    "\n",
    "    print(\"Testing Accuracy: \", network.predict(testing_data).eq(testing_data_label).float().mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3feabafda0091d4a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
