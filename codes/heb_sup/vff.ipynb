{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-26T06:59:11.279641Z",
     "start_time": "2024-08-26T06:58:44.789920700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from matplotlib) (4.50.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from matplotlib) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from matplotlib) (6.3.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.18.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Training Data:  tensor([[-1.2854, -1.5955, -1.4598,  ...,  0.5100, -0.5825, -0.8167],\n",
      "        [ 0.5562,  0.0134, -0.3936,  ...,  0.4905,  0.5491,  0.5881],\n",
      "        [ 2.5141,  2.4753,  2.4753,  ..., -0.6020, -0.6020, -0.5825],\n",
      "        ...,\n",
      "        [ 0.9633,  1.4479,  1.5836,  ...,  0.1979, -1.2654, -1.5971],\n",
      "        [-1.8863, -1.2854, -1.1303,  ..., -1.5190, -1.3629, -1.4215],\n",
      "        [-1.2272, -1.6537, -1.2466,  ...,  0.2564,  0.1979,  0.0418]])\n",
      "Training Data Label:  tensor([6, 9, 9,  ..., 3, 7, 1])\n",
      "Positive Data:  tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.5100, -0.5825, -0.8167],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.4905,  0.5491,  0.5881],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ..., -0.6020, -0.6020, -0.5825],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.1979, -1.2654, -1.5971],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ..., -1.5190, -1.3629, -1.4215],\n",
      "        [ 0.0000,  1.0000,  0.0000,  ...,  0.2564,  0.1979,  0.0418]],\n",
      "       device='cuda:0')\n",
      "Negative Data:  tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.5100, -0.5825, -0.8167],\n",
      "        [ 0.0000,  1.0000,  0.0000,  ...,  0.4905,  0.5491,  0.5881],\n",
      "        [ 1.0000,  0.0000,  0.0000,  ..., -0.6020, -0.6020, -0.5825],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.1979, -1.2654, -1.5971],\n",
      "        [ 0.0000,  1.0000,  0.0000,  ..., -1.5190, -1.3629, -1.4215],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.2564,  0.1979,  0.0418]],\n",
      "       device='cuda:0')\n",
      "Training Layer 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 42.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Layer 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 20.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.2919999957084656\n",
      "Testing Accuracy:  0.2861666679382324\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n",
    "!pip install matplotlib\n",
    "\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST, FashionMNIST, CIFAR10\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Lambda\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch import Tensor\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import Adam\n",
    "\n",
    "number_of_epochs = 20\n",
    "\n",
    "def regularize_matrix(A, lambda_val=1e-5):\n",
    "    n = A.size(0)  # Assuming A is square\n",
    "    I = torch.eye(n, dtype=A.dtype, device=A.device)\n",
    "    A_regularized = A + lambda_val * I\n",
    "    return A_regularized\n",
    "\n",
    "def add_noise(A, epsilon=1e-5):\n",
    "    noise = (torch.randn(A.shape) * epsilon).cuda()\n",
    "    return A + noise\n",
    "\n",
    "def find_particular_solution(A, b):\n",
    "    A_pinv = torch.linalg.pinv(A)  # Compute pseudoinverse\n",
    "    x_p = A_pinv @ b\n",
    "    return x_p\n",
    "\n",
    "class Layer(nn.Linear):\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True, device=None, d_type=None, is_hinge_loss=False):\n",
    "        super().__init__(in_features, out_features, bias, device, d_type)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.learning_rate = 0.2\n",
    "        self.optimizer = Adam(self.parameters(), lr=self.learning_rate)\n",
    "        # self.layer_weights = nn.Parameter(torch.ones(6, 800))\n",
    "        self.threshold = 3.0\n",
    "        self.num_of_epochs = number_of_epochs\n",
    "        self.is_hinge_loss = is_hinge_loss\n",
    "        \n",
    "        \n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        normalized_input = input / (input.norm(2, 1, keepdim=True) + 1e-4)\n",
    "        output = torch.mm(normalized_input, self.weight.T) + self.bias.unsqueeze(0)\n",
    "        return self.activation(output)\n",
    "\n",
    "\n",
    "    def balanced_loss(self, positive_goodness, negative_goodness, alpha=4.0):\n",
    "        delta = positive_goodness - negative_goodness\n",
    "        per_instance_loss = torch.log(1 + torch.exp(-alpha * delta))\n",
    "        return per_instance_loss.mean()  # Averaging the loss to ensure it's a scalar\n",
    "\n",
    "\n",
    "    def soft_plus_loss(self, positive_goodness, negative_goodness, is_second_phase=False):\n",
    "        if is_second_phase:\n",
    "            threshold = self.threshold * 2\n",
    "        else:\n",
    "            threshold = self.threshold\n",
    "        return torch.log(1 + torch.exp(torch.cat([\n",
    "            -positive_goodness + threshold,\n",
    "            negative_goodness - threshold]))).mean()\n",
    "\n",
    "\n",
    "\n",
    "    def plot_goodness(self, positive_goodness_history, negative_goodness_history,\n",
    "                      positive_unaltered_goodness_history, negative_unaltered_goodness_history):\n",
    "        epochs = range(1, self.num_of_epochs + 1)\n",
    "\n",
    "        plt.figure(figsize=(14, 7))\n",
    "\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(epochs, positive_goodness_history, label='Altered Positive Goodness')\n",
    "        plt.plot(epochs, positive_unaltered_goodness_history, label='Unaltered Positive Goodness')\n",
    "        plt.legend()\n",
    "        plt.title('Positive Goodness Comparison Over Epochs')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Goodness')\n",
    "\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(epochs, negative_goodness_history, label='Altered Negative Goodness')\n",
    "        plt.plot(epochs, negative_unaltered_goodness_history, label='Unaltered Negative Goodness')\n",
    "        plt.legend()\n",
    "        plt.title('Negative Goodness Comparison Over Epochs')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Goodness')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def train_layer(self, positive_input, negative_input, layer_num):\n",
    "\n",
    "        for _ in tqdm(range(self.num_of_epochs)):\n",
    "            positive_output = self.forward(positive_input)  # Shape: [batch_size, 500]\n",
    "            negative_output = self.forward(negative_input)\n",
    "    \n",
    "            positive_goodness = (positive_output.pow(2)).mean(1)  # Shape: [batch_size]\n",
    "            negative_goodness = (negative_output.pow(2)).mean(1)\n",
    "\n",
    "            if self.is_hinge_loss:\n",
    "                loss = self.soft_plus_loss(positive_goodness, negative_goodness)\n",
    "            else:\n",
    "                loss = self.balanced_loss(positive_goodness, negative_goodness)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        # layer_weight_row = Layer.layer_weights[layer_num, :]\n",
    "        # layer_weight_four_row = Layer.layer_weights_four[layer_num, :]\n",
    "        # positive_output = self.forward(positive_input)\n",
    "        # negative_output = self.forward(negative_input)\n",
    "        \n",
    "        # if layer_num == 0:\n",
    "        #     # Layer.layer_values_one = torch.cat((positive_output.pow(2) * layer_weight_row + positive_output.pow(4) * layer_weight_four_row, negative_output.pow(2) * layer_weight_row + negative_output.pow(4) * layer_weight_four_row), dim=0)\n",
    "        #     # \n",
    "        #     new_values_one = (positive_output.pow(2) * layer_weight_row + positive_output.pow(4) * layer_weight_four_row).mean(1).detach().clone()\n",
    "        #     print(f\"SHAPE: {new_values_one.shape}\")\n",
    "        #     new_values_two = (negative_output.pow(2) * layer_weight_row + negative_output.pow(4) * layer_weight_four_row).mean(1).detach().clone()\n",
    "        #     Layer.layer_one_sum = torch.cat((new_values_one, new_values_two), dim=0)\n",
    "        #     \n",
    "        # \n",
    "        #     \n",
    "        # else:\n",
    "        #     # Layer.layer_values_two = torch.cat((positive_output.pow(2) * layer_weight_row + positive_output.pow(4) * layer_weight_four_row, negative_output.pow(2) * layer_weight_row + negative_output.pow(4) * layer_weight_four_row), dim=0)\n",
    "        #     \n",
    "        #     new_values_one = (positive_output.pow(2) * layer_weight_row + positive_output.pow(4) * layer_weight_four_row).mean(1).detach().clone()\n",
    "        #     new_values_two = (negative_output.pow(2) * layer_weight_row + negative_output.pow(4) * layer_weight_four_row).mean(1).detach().clone()\n",
    "        #     Layer.layer_two_sum = torch.cat((new_values_one, new_values_two), dim=0)\n",
    "        #     \n",
    "        return self.forward(positive_input).detach(), self.forward(negative_input).detach()\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self, dimension_configs):\n",
    "        super().__init__()\n",
    "        self.layers = []\n",
    "        #self.layer_weights = nn.Parameter(torch.ones(2))\n",
    "        for i in range(len(dimension_configs) - 1):\n",
    "            self.layers += [Layer(dimension_configs[i], dimension_configs[i + 1]).cuda()]\n",
    "\n",
    "    def mark_data(self, data, label):\n",
    "        marked_data = data.clone().cuda()\n",
    "        marked_data[:, :10] = 0\n",
    "        marked_data[torch.arange(marked_data.size(0)), label] = 1\n",
    "        return marked_data\n",
    "# Layer.layer_final_weights[layer_num]\n",
    "    def predict(self, input_data):\n",
    "        goodness_per_label = []\n",
    "        for label in range(10):\n",
    "            marked_data = self.mark_data(input_data, label)\n",
    "            goodness = []\n",
    "            for layer_num, layer in enumerate(self.layers):\n",
    "                marked_data = layer(marked_data)\n",
    "             \n",
    "\n",
    "                goodness_value = ((marked_data.pow(2)) * 1).mean(1)\n",
    "                goodness.append(goodness_value)\n",
    "            goodness_per_label.append(torch.sum(torch.stack(goodness), dim=0).unsqueeze(1))\n",
    "        goodness_per_label = torch.cat(goodness_per_label, 1)\n",
    "        return goodness_per_label.argmax(dim=1)\n",
    "\n",
    "    def train_network(self, positive_goodness, negative_goodness):\n",
    "        #optimizer = Adam(self.parameters(), lr=0.001)\n",
    "        goodness_pos, goodness_neg = positive_goodness, negative_goodness\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print('Training Layer', i, '...')\n",
    "            goodness_pos, goodness_neg = layer.train_layer(goodness_pos, goodness_neg, i)\n",
    "            \n",
    "        # print(Layer.layer_one_sum)\n",
    "        # data_one = Layer.layer_one_sum\n",
    "        # data_two = Layer.layer_two_sum\n",
    "        # \n",
    "        # combined_output = torch.cat((data_one.unsqueeze(1), data_two.unsqueeze(1)), dim=1)\n",
    "        # positive_labels = torch.full((60000, 1), 9999999, dtype=torch.float32)\n",
    "        # negative_labels = torch.full((60000, 1), -9999999, dtype=torch.float32)\n",
    "        # labels = torch.cat((positive_labels, negative_labels), dim=0).cuda()\n",
    "        # \n",
    "        # A = combined_output.cuda()\n",
    "        # b = labels.cuda()\n",
    "        # x = torch.linalg.pinv(A) @ b\n",
    "        # \n",
    "        # new_weights = x.reshape(2).cuda()\n",
    "        # print(f\"NEW WEIGHTS: {new_weights}\")\n",
    "        # Layer.layer_final_weights = new_weights.clone()\n",
    "        # \n",
    "        # print(f\"FINAL WEIGHTS: {Layer.layer_final_weights}\")\n",
    "        # combined_output = torch.cat((Layer.layer_values_one, Layer.layer_values_two), dim=1).cuda()\n",
    "        # positive_labels = torch.full((60000, 1), 9999999, dtype=torch.float32)\n",
    "        # negative_labels = torch.zeros((60000, 1), dtype=torch.float32)\n",
    "        # labels = torch.cat((positive_labels, negative_labels), dim=0).cuda()\n",
    "        # A = combined_output.cuda()\n",
    "        # b = labels.cuda()\n",
    "        # x = torch.linalg.pinv(A) @ b\n",
    "        # \n",
    "        # new_weights = x.reshape(1, 800).cuda()\n",
    "        # \n",
    "        # Layer.layer_final_weights.data[0, :] = new_weights[0, :400].clone()\n",
    "        # Layer.layer_final_weights.data[1, :] = new_weights[0, 400:800].clone()\n",
    "\n",
    "\n",
    "def load_CIFAR10_data(train_batch_size=30000, test_batch_size=6000):\n",
    "    data_transformation = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)), \n",
    "        Lambda(lambda x: torch.flatten(x))\n",
    "    ])\n",
    "\n",
    "    training_data_loader = DataLoader(\n",
    "        CIFAR10('./data/', train=True, download=True, transform=data_transformation),\n",
    "        batch_size=train_batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    testing_data_loader = DataLoader(\n",
    "        CIFAR10('./data/', train=False, download=True, transform=data_transformation),\n",
    "        batch_size=test_batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    return training_data_loader, testing_data_loader\n",
    "\n",
    "def load_FashionMNIST_data(train_batch_size=60000, test_batch_size=10000):\n",
    "    data_transformation = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize((0.2860,), (0.3530,)),\n",
    "        Lambda(lambda x: torch.flatten(x))\n",
    "    ])\n",
    "\n",
    "    training_data_loader = DataLoader(\n",
    "        FashionMNIST('./data/', train=True, download=True, transform=data_transformation),\n",
    "        batch_size=train_batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    testing_data_loader = DataLoader(\n",
    "        FashionMNIST('./data/', train=False, download=True, transform=data_transformation),\n",
    "        batch_size=test_batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    return training_data_loader, testing_data_loader\n",
    "\n",
    "def load_MNIST_data(train_batch_size=50000, test_batch_size=10000):\n",
    "    data_transformation = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize((0.1307,), (0.3081,)),\n",
    "        Lambda(lambda x: torch.flatten(x))\n",
    "    ])\n",
    "\n",
    "    training_data_loader = DataLoader(\n",
    "        MNIST('./data/', train=True, download=True, transform=data_transformation),\n",
    "        batch_size=train_batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    testing_data_loader = DataLoader(\n",
    "        MNIST('./data/', train=False, download=True, transform=data_transformation),\n",
    "        batch_size=test_batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    return training_data_loader, testing_data_loader\n",
    "\n",
    "\n",
    "def create_positive_data(data, label):\n",
    "    positive_data = data.clone()\n",
    "    positive_data[:, :10] = 0.0\n",
    "\n",
    "    for i in range(positive_data.shape[0]):\n",
    "        positive_data[i][label[i]] = 1.0\n",
    "\n",
    "    return positive_data\n",
    "\n",
    "\n",
    "def create_negative_data(data, label, seed=None):\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "\n",
    "    negative_data = data.clone()\n",
    "    negative_data[:, :10] = 0.0\n",
    "\n",
    "    for i in range(negative_data.shape[0]):\n",
    "        possible_answers = list(range(10))\n",
    "        possible_answers.remove(label[i])\n",
    "        false_label = random.choice(possible_answers)\n",
    "        negative_data[i][false_label] = 1.0\n",
    "\n",
    "    return negative_data\n",
    "\n",
    "\n",
    "def prepare_data():\n",
    "    torch.manual_seed(1234)\n",
    "    training_data_loader, testing_data_loader = load_CIFAR10_data()\n",
    "\n",
    "    training_data, training_data_label = next(iter(training_data_loader))\n",
    "\n",
    "    testing_data, testing_data_label = next(iter(testing_data_loader))\n",
    "    testing_data, testing_data_label = testing_data.cuda(), testing_data_label.cuda()\n",
    "\n",
    "    print(f\"Training Data: \", training_data)\n",
    "    print(f\"Training Data Label: \", training_data_label)\n",
    "\n",
    "    training_data, training_data_label = training_data.cuda(), training_data_label.cuda()\n",
    "\n",
    "    positive_data = create_positive_data(training_data, training_data_label)\n",
    "    print(f\"Positive Data: \", positive_data)\n",
    "\n",
    "    negative_data = create_negative_data(training_data, training_data_label, seed=1234)\n",
    "    print(f\"Negative Data: \", negative_data)\n",
    "\n",
    "    return positive_data, negative_data, training_data, training_data_label, testing_data, testing_data_label\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.manual_seed(1234)\n",
    "    positive_data, negative_data, training_data, training_data_label, testing_data, testing_data_label = prepare_data()\n",
    "    network = Network([3072, 1000, 1000]).cuda() #3072\n",
    "    network.train_network(positive_data, negative_data)\n",
    "\n",
    "    print(\"Training Accuracy: \", network.predict(training_data).eq(training_data_label).float().mean().item())\n",
    "\n",
    "    print(\"Testing Accuracy: \", network.predict(testing_data).eq(testing_data_label).float().mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-26T06:52:59.594737700Z",
     "start_time": "2024-08-26T06:52:59.581745800Z"
    }
   },
   "id": "b15b7aaf494a3c66"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e4f8f091cd0a3383"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
