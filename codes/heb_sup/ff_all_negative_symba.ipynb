{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "!pip install tqdm\n",
    "!pip install matplotlib\n",
    "\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST, FashionMNIST, CIFAR10\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Lambda\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch import Tensor\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import Adam\n",
    "\n",
    "number_of_epochs = 1000\n",
    "\n",
    "hidden_layer_neurons = 1000\n",
    "\n",
    "class Layer(nn.Linear):\n",
    "    # hebbian_weights_layer_one_zeroth = nn.Parameter(torch.ones(10, hidden_layer_neurons).cuda())\n",
    "    # hebbian_weights_layer_two_zeroth = nn.Parameter(torch.ones(10, hidden_layer_neurons).cuda())\n",
    "    hebbian_weights_layer_one = nn.Parameter(torch.ones(10, hidden_layer_neurons).cuda())\n",
    "    # hebbian_weights_layer_one_quadra = nn.Parameter(torch.ones(10, hidden_layer_neurons).cuda())\n",
    "    hebbian_weights_layer_two = nn.Parameter(torch.ones(10, hidden_layer_neurons).cuda())\n",
    "    # hebbian_weights_layer_two_quadra = nn.Parameter(torch.ones(10, hidden_layer_neurons).cuda())\n",
    "    # hebbian_weights_layer_one_third = nn.Parameter(torch.ones(10, hidden_layer_neurons).cuda())\n",
    "    # hebbian_weights_layer_two_third = nn.Parameter(torch.ones(10, hidden_layer_neurons).cuda())\n",
    "    # \n",
    "    def __init__(self, in_features, out_features, bias=True, device=None, d_type=None, is_hinge_loss=False):\n",
    "        super().__init__(in_features, out_features, bias, device, d_type)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.learning_rate = 0.01\n",
    "        self.optimizer = Adam(self.parameters(), lr=self.learning_rate)\n",
    "        self.threshold = 0.0\n",
    "        self.num_of_epochs = number_of_epochs\n",
    "        self.is_hinge_loss = is_hinge_loss\n",
    "        self.hebbian_optimizer = Adam([Layer.hebbian_weights_layer_two, Layer.hebbian_weights_layer_one], lr=0.01)\n",
    "        \n",
    "        \n",
    "    def compute_hebbian_activity(self, labels, values, layer_num):\n",
    "        \n",
    "        \n",
    "        if layer_num == 0:\n",
    "            hebbian_value = torch.mm(values.pow(2) * self.layer_weights[layer_num], self.hebbian_weights_layer_one.T) * labels\n",
    "        else:\n",
    "            hebbian_value = torch.mm(values, self.hebbian_weights_layer_two.T) * labels\n",
    "        return hebbian_value.mean(1).cuda()\n",
    "    \n",
    "    \n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        normalized_input = input / (input.norm(2, 1, keepdim=True) + 1e-4)\n",
    "        output = torch.mm(normalized_input, self.weight.T) + self.bias.unsqueeze(0)\n",
    "        return self.activation(output)\n",
    "\n",
    "    def balanced_loss(self, positive_goodness, negative_goodness, alpha=4.0):\n",
    "        delta = positive_goodness - negative_goodness\n",
    "        per_instance_loss = torch.log(1 + torch.exp(-alpha * delta))\n",
    "        return per_instance_loss.mean()  \n",
    "\n",
    "\n",
    "    def soft_plus_loss(self, positive_goodness, negative_goodness, is_second_phase=False):\n",
    "        if is_second_phase:\n",
    "            threshold = self.threshold * 2\n",
    "        else:\n",
    "            threshold = self.threshold\n",
    "        return torch.log(1 + torch.exp(torch.cat([\n",
    "            -positive_goodness + threshold,\n",
    "            negative_goodness - threshold]))).mean()\n",
    "\n",
    "\n",
    "    def plot_goodness(self, positive_goodness, negative_goodness, difference):\n",
    "        plt.figure(figsize=(10, 5), dpi=400)  # Higher dpi for better resolution\n",
    "        plt.plot(positive_goodness, label='Positive Goodness', color='b')\n",
    "        plt.plot(negative_goodness, label='Negative Goodness', color='r')\n",
    "        plt.plot(difference, label='Difference Goodness', color='g')\n",
    "        plt.xlabel('Epoch', fontsize=14)\n",
    "        plt.ylabel('Goodness Value', fontsize=14)\n",
    "        plt.title('Change in Goodness During Training', fontsize=16)\n",
    "        plt.legend(fontsize=12)\n",
    "        plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "        plt.tick_params(labelsize=12)\n",
    "        plt.tight_layout()  # Adjusts plot to ensure everything fits without overlap\n",
    "        plt.savefig('Goodness_Tracking.pdf', format='pdf')  # Save as PDF\n",
    "        plt.show()\n",
    "\n",
    "    def plot_difference(self, difference):\n",
    "        plt.figure(figsize=(10, 5), dpi=400)\n",
    "        plt.plot(difference, label='Difference Goodness', color='g')\n",
    "        plt.xlabel('Epoch', fontsize=14)\n",
    "        plt.ylabel('Goodness Value', fontsize=14)\n",
    "        plt.title('Change in Goodness During Training', fontsize=16)\n",
    "        plt.legend(fontsize=12)\n",
    "        plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "        plt.tick_params(labelsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('Difference_Goodness.pdf', format='pdf')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def train_layer(self, positive_input, negative_input, layer_num):\n",
    "        positive_goodness_history = []\n",
    "        negative_goodness_history = []\n",
    "        difference_history = []\n",
    "        \n",
    "        for _ in range(number_of_epochs):\n",
    "            positive_output = self.forward(positive_input)  # Shape: [batch_size, 500]\n",
    "            negative_output = self.forward(negative_input)\n",
    "            \n",
    "            # First Layer\n",
    "            if layer_num == 0:\n",
    "                # positive_goodness = (torch.mm(Network.positive_labels, Layer.hebbian_weights_layer_one_zeroth) * positive_output + torch.mm(Network.positive_labels, Layer.hebbian_weights_layer_one) * positive_output.pow(2) + torch.mm(Network.positive_labels, Layer.hebbian_weights_layer_one_quadra) * positive_output.pow(4) + torch.mm(Network.positive_labels, Layer.hebbian_weights_layer_one_third) * positive_output.pow(3)).mean(1)\n",
    "                positive_goodness = (torch.mm(Network.positive_labels, Layer.hebbian_weights_layer_one) * positive_output.pow(2)).mean(1)\n",
    "                # negative_goodness = (torch.mm(Network.negative_labels, Layer.hebbian_weights_layer_one_zeroth) * negative_output + torch.mm(Network.negative_labels, Layer.hebbian_weights_layer_one) * negative_output.pow(2) + torch.mm(Network.negative_labels, Layer.hebbian_weights_layer_one_quadra) * negative_output.pow(4) + torch.mm(Network.negative_labels, Layer.hebbian_weights_layer_one_third) * negative_output.pow(3)).mean(1)\n",
    "                negative_goodness = (torch.mm(Network.negative_labels, Layer.hebbian_weights_layer_one) * negative_output.pow(2)).mean(1)\n",
    "            else:\n",
    "                # Second Layer\n",
    "                # positive_goodness = (torch.mm(Network.positive_labels, Layer.hebbian_weights_layer_two_zeroth) * positive_output + torch.mm(Network.positive_labels, Layer.hebbian_weights_layer_two) * positive_output.pow(2) + torch.mm(Network.positive_labels, Layer.hebbian_weights_layer_two_quadra) * positive_output.pow(4) + torch.mm(Network.positive_labels, Layer.hebbian_weights_layer_two_third) * positive_output.pow(3)).mean(1)\n",
    "                positive_goodness = (torch.mm(Network.positive_labels, Layer.hebbian_weights_layer_two) * positive_output.pow(2)).mean(1)\n",
    "                # negative_goodness = (torch.mm(Network.negative_labels, Layer.hebbian_weights_layer_two_zeroth) * positive_output + torch.mm(Network.negative_labels, Layer.hebbian_weights_layer_two) * negative_output.pow(2) + torch.mm(Network.negative_labels, Layer.hebbian_weights_layer_two_quadra) * negative_output.pow(4) + torch.mm(Network.negative_labels, Layer.hebbian_weights_layer_two_third) * negative_output.pow(3)).mean(1)\n",
    "                negative_goodness = (torch.mm(Network.negative_labels, Layer.hebbian_weights_layer_two) * negative_output.pow(2)).mean(1)\n",
    "            \n",
    "            positive_goodness_history.append(positive_goodness.mean().item())\n",
    "            negative_goodness_history.append(negative_goodness.mean().item())\n",
    "            difference_history.append(positive_goodness.mean().item() - negative_goodness.mean().item())\n",
    "\n",
    "            if self.is_hinge_loss:\n",
    "                loss = self.soft_plus_loss(positive_goodness, negative_goodness)\n",
    "            else:\n",
    "                loss = self.balanced_loss(positive_goodness, negative_goodness)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            self.hebbian_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.hebbian_optimizer.step()\n",
    "        \n",
    "        self.plot_goodness(positive_goodness_history, negative_goodness_history, difference_history)\n",
    "        self.plot_difference(difference_history)\n",
    "        return self.forward(positive_input).detach(), self.forward(negative_input).detach()\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "    # hebbian_weights = nn.Parameter(torch.ones(10, 774).cuda())\n",
    "    positive_labels = []\n",
    "    negative_labels= []\n",
    "    def __init__(self, dimension_configs):\n",
    "        super().__init__()\n",
    "        self.layers = []\n",
    "        for i in range(len(dimension_configs) - 1):\n",
    "            self.layers += [Layer(dimension_configs[i], dimension_configs[i + 1]).cuda()]\n",
    "            \n",
    "    def balanced_loss(self, positive_goodness, negative_goodness, alpha=4.0):\n",
    "        delta = positive_goodness - negative_goodness\n",
    "        per_instance_loss = torch.log(1 + torch.exp(-alpha * delta))\n",
    "        return per_instance_loss.mean()  \n",
    "\n",
    "    def mark_data(self, data, label):\n",
    "        marked_data = data.clone().cuda()\n",
    "        marked_data[:, :10] = 0\n",
    "        marked_data[torch.arange(marked_data.size(0)), label] = 1\n",
    "        return marked_data\n",
    "    \n",
    "    \n",
    "    def predict(self, input_data):\n",
    "        goodness_per_label = []\n",
    "        for label in range(10):\n",
    "            marked_data = self.mark_data(input_data, label)\n",
    "            # hebbian_weight_one_zeroth = Layer.hebbian_weights_layer_one_zeroth[label, :]\n",
    "            # hebbian_weight_two_zeroth = Layer.hebbian_weights_layer_two_zeroth[label, :]\n",
    "            hebbian_weight_one = Layer.hebbian_weights_layer_one[label, :]\n",
    "            hebbian_weight_two = Layer.hebbian_weights_layer_two[label, :]\n",
    "            # hebbian_weight_one_third = Layer.hebbian_weights_layer_one_third[label, :]\n",
    "            # hebbian_weight_two_third = Layer.hebbian_weights_layer_two_third[label, :]\n",
    "            # hebbian_weight_one_quadra = Layer.hebbian_weights_layer_one_quadra[label, :]\n",
    "            # hebbian_weight_two_quadra = Layer.hebbian_weights_layer_two_quadra[label, :]\n",
    "            goodness = []\n",
    "            for layer_num, layer in enumerate(self.layers):\n",
    "                marked_data = layer(marked_data)\n",
    "                if layer_num == 0:\n",
    "                    goodness_value = (marked_data.pow(2) * hebbian_weight_one).mean(1)\n",
    "                else:\n",
    "                    goodness_value = (marked_data.pow(2) * hebbian_weight_two).mean(1)\n",
    "                goodness.append(goodness_value)\n",
    "\n",
    "            goodness_per_label.append(torch.sum(torch.stack(goodness), dim=0).unsqueeze(1))\n",
    "        goodness_per_label = torch.cat(goodness_per_label, 1)\n",
    "        return goodness_per_label.argmax(dim=1)\n",
    "    \n",
    "    def compute_hebbian_activity(self, values):\n",
    "        labels = values[:, :10].cuda()\n",
    "        hebbian = values[:, 10:].cuda()\n",
    "        hebbian_value = torch.mm(hebbian, self.hebbian_weights.T) * labels\n",
    "        return hebbian_value.mean(1).cuda()\n",
    "    \n",
    "    \n",
    "    def train_network(self, positive_goodness, negative_goodness, training_data, training_data_label):\n",
    "        \n",
    "        # positive_mean_values = []\n",
    "        # negative_mean_values = []\n",
    "        # \n",
    "        # for _ in tqdm(range(number_of_epochs)):\n",
    "        #     positive_labels = positive_goodness[:, :10].cuda()\n",
    "        #     positive_hebbian = positive_goodness[:, 10:].cuda()\n",
    "        #     positive_hebbian_value = torch.mm(positive_hebbian, self.hebbian_weights.T.pow(2)) * positive_labels\n",
    "        #     positive_mean_value = positive_hebbian_value.mean(1).cuda()\n",
    "        #     positive_mean_values.append(positive_mean_value.mean().item())  # Store average for plotting\n",
    "        # \n",
    "        #     negative_labels = negative_goodness[:, :10].cuda()\n",
    "        #     negative_hebbian = negative_goodness[:, 10:].cuda()\n",
    "        #     negative_hebbian_value = torch.mm(negative_hebbian, self.hebbian_weights.T.pow(2)) * negative_labels\n",
    "        #     negative_mean_value = negative_hebbian_value.mean(1).cuda()\n",
    "        #     negative_mean_values.append(negative_mean_value.mean().item())  # Store average for plotting\n",
    "        # \n",
    "        #     loss = self.balanced_loss(positive_mean_value, negative_mean_value)\n",
    "        #     self.hebbian_optimizer.zero_grad()\n",
    "        #     loss.backward()\n",
    "        #     self.hebbian_optimizer.step()\n",
    "        # \n",
    "        # plt.figure(figsize=(10, 5))\n",
    "        # plt.plot(positive_mean_values, label='Positive Mean Value', color='blue')\n",
    "        # plt.plot(negative_mean_values, label='Negative Mean Value', color='red')\n",
    "        # plt.title('Change in Mean Values Over Epochs')\n",
    "        # plt.xlabel('Epochs')\n",
    "        # plt.ylabel('Mean Value')\n",
    "        # plt.legend()\n",
    "        # plt.grid(True)\n",
    "        # plt.show()\n",
    "        # \n",
    "        for epoch in tqdm(range(1)):\n",
    "            goodness_pos, goodness_neg = positive_goodness, negative_goodness\n",
    "            negative_goodness = create_negative_data(training_data, training_data_label)\n",
    "            goodness_neg = negative_goodness\n",
    "            positive_labels = goodness_pos[:, :10]\n",
    "            negative_labels = negative_goodness[:, :10]\n",
    "\n",
    "            Network.positive_labels = nn.Parameter(positive_labels.cuda())\n",
    "            Network.negative_labels = nn.Parameter(negative_labels.cuda())\n",
    "            for i, layer in enumerate(self.layers):\n",
    "                print('Training Layer', i, '...')\n",
    "                goodness_pos, goodness_neg = layer.train_layer(goodness_pos, goodness_neg, i)\n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "def load_CIFAR10_data(train_batch_size=30000, test_batch_size=6000):\n",
    "    data_transformation = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)), \n",
    "        Lambda(lambda x: torch.flatten(x))\n",
    "    ])\n",
    "\n",
    "    training_data_loader = DataLoader(\n",
    "        CIFAR10('./data/', train=True, download=True, transform=data_transformation),\n",
    "        batch_size=train_batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    testing_data_loader = DataLoader(\n",
    "        CIFAR10('./data/', train=False, download=True, transform=data_transformation),\n",
    "        batch_size=test_batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    return training_data_loader, testing_data_loader\n",
    "\n",
    "def load_FashionMNIST_data(train_batch_size=60000, test_batch_size=10000):\n",
    "    data_transformation = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize((0.2860,), (0.3530,)),\n",
    "        Lambda(lambda x: torch.flatten(x))\n",
    "    ])\n",
    "\n",
    "    training_data_loader = DataLoader(\n",
    "        FashionMNIST('./data/', train=True, download=True, transform=data_transformation),\n",
    "        batch_size=train_batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    testing_data_loader = DataLoader(\n",
    "        FashionMNIST('./data/', train=False, download=True, transform=data_transformation),\n",
    "        batch_size=test_batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    return training_data_loader, testing_data_loader\n",
    "\n",
    "def load_MNIST_data(train_batch_size=50000, test_batch_size=10000):\n",
    "    data_transformation = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize((0.1307,), (0.3081,)),\n",
    "        Lambda(lambda x: torch.flatten(x))\n",
    "    ])\n",
    "\n",
    "    training_data_loader = DataLoader(\n",
    "        MNIST('./data/', train=True, download=True, transform=data_transformation),\n",
    "        batch_size=train_batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    testing_data_loader = DataLoader(\n",
    "        MNIST('./data/', train=False, download=True, transform=data_transformation),\n",
    "        batch_size=test_batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    return training_data_loader, testing_data_loader\n",
    "\n",
    "\n",
    "def create_positive_data(data, label):\n",
    "    positive_data = data.clone()\n",
    "    positive_data[:, :10] = 0.0\n",
    "\n",
    "    for i in range(positive_data.shape[0]):\n",
    "        positive_data[i][label[i]] = 1.0\n",
    "\n",
    "    return positive_data\n",
    "\n",
    "\n",
    "def create_negative_data(data, label, seed=None):\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "\n",
    "    negative_data = data.clone()\n",
    "    negative_data[:, :10] = 0.0\n",
    "\n",
    "    for i in range(negative_data.shape[0]):\n",
    "        possible_answers = list(range(10))\n",
    "        possible_answers.remove(label[i])\n",
    "        false_label = random.choice(possible_answers)\n",
    "        negative_data[i][false_label] = 1.0\n",
    "\n",
    "    return negative_data\n",
    "\n",
    "\n",
    "\n",
    "def prepare_data():\n",
    "    torch.manual_seed(4321)\n",
    "    training_data_loader, testing_data_loader = load_CIFAR10_data()\n",
    "\n",
    "    training_data, training_data_label = next(iter(training_data_loader))\n",
    "\n",
    "    testing_data, testing_data_label = next(iter(testing_data_loader))\n",
    "    testing_data, testing_data_label = testing_data.cuda(), testing_data_label.cuda()\n",
    "\n",
    "    print(f\"Training Data: \", training_data)\n",
    "    print(f\"Training Data Label: \", training_data_label)\n",
    "\n",
    "    training_data, training_data_label = training_data.cuda(), training_data_label.cuda()\n",
    "\n",
    "    positive_data = create_positive_data(training_data, training_data_label)\n",
    "    print(f\"Positive Data: \", positive_data)\n",
    "\n",
    "    negative_data = create_negative_data(training_data, training_data_label)\n",
    "    print(f\"Negative Data: \", negative_data)\n",
    "\n",
    "    return positive_data, negative_data, training_data, training_data_label, testing_data, testing_data_label\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.manual_seed(1234)\n",
    "    positive_data, negative_data, training_data, training_data_label, testing_data, testing_data_label = prepare_data()\n",
    "    network = Network([3072, hidden_layer_neurons, hidden_layer_neurons]).cuda() #3072\n",
    "    network.train_network(positive_data, negative_data, training_data, training_data_label)\n",
    "    print(\"Training Accuracy: \", network.predict(training_data).eq(training_data_label).float().mean().item())\n",
    "    print(\"Testing Accuracy: \", network.predict(testing_data).eq(testing_data_label).float().mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "fc13d8a9f8a56b57"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
