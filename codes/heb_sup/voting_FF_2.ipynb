{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-08-24T06:14:02.410803100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from matplotlib) (4.50.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from matplotlib) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from matplotlib) (6.3.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.18.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\study notes\\part iv project\\backpropagation-free-research\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Training Data:  tensor([[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "        [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "        [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "        ...,\n",
      "        [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "        [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "        [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]])\n",
      "Training Data Label:  tensor([5, 0, 4,  ..., 8, 4, 8])\n",
      "Positive Data:  tensor([[ 0.0000,  0.0000,  0.0000,  ..., -0.4242, -0.4242, -0.4242],\n",
      "        [ 1.0000,  0.0000,  0.0000,  ..., -0.4242, -0.4242, -0.4242],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ..., -0.4242, -0.4242, -0.4242],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ..., -0.4242, -0.4242, -0.4242],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ..., -0.4242, -0.4242, -0.4242],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ..., -0.4242, -0.4242, -0.4242]],\n",
      "       device='cuda:0')\n",
      "Negative Data:  tensor([[ 0.0000,  0.0000,  0.0000,  ..., -0.4242, -0.4242, -0.4242],\n",
      "        [ 0.0000,  0.0000,  1.0000,  ..., -0.4242, -0.4242, -0.4242],\n",
      "        [ 1.0000,  0.0000,  0.0000,  ..., -0.4242, -0.4242, -0.4242],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ..., -0.4242, -0.4242, -0.4242],\n",
      "        [ 1.0000,  0.0000,  0.0000,  ..., -0.4242, -0.4242, -0.4242],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ..., -0.4242, -0.4242, -0.4242]],\n",
      "       device='cuda:0')\n",
      "Training Layer 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 803/1000 [07:51<01:51,  1.76it/s]"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n",
    "!pip install matplotlib\n",
    "\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST, FashionMNIST, CIFAR10\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Lambda\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch import Tensor\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import Adam\n",
    "\n",
    "number_of_epochs = 1000\n",
    "\n",
    "hidden_layer_neurons = 500\n",
    "\n",
    "class Layer(nn.Linear):\n",
    "    # hebbian_weights_layer_one_zeroth = nn.Parameter(torch.ones(10, hidden_layer_neurons).cuda())\n",
    "    # hebbian_weights_layer_two_zeroth = nn.Parameter(torch.ones(10, hidden_layer_neurons).cuda())\n",
    "    hebbian_weights_layer_one = nn.Parameter(torch.ones(10, hidden_layer_neurons).cuda())\n",
    "    # hebbian_weights_layer_one_quadra = nn.Parameter(torch.ones(10, hidden_layer_neurons).cuda())\n",
    "    hebbian_weights_layer_two = nn.Parameter(torch.ones(10, hidden_layer_neurons).cuda())\n",
    "    # hebbian_weights_layer_two_quadra = nn.Parameter(torch.ones(10, hidden_layer_neurons).cuda())\n",
    "    # hebbian_weights_layer_one_third = nn.Parameter(torch.ones(10, hidden_layer_neurons).cuda())\n",
    "    # hebbian_weights_layer_two_third = nn.Parameter(torch.ones(10, hidden_layer_neurons).cuda())\n",
    "    # \n",
    "    def __init__(self, in_features, out_features, bias=True, device=None, d_type=None, is_hinge_loss=False):\n",
    "        super().__init__(in_features, out_features, bias, device, d_type)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.learning_rate = 0.02\n",
    "        self.optimizer = Adam(self.parameters(), lr=self.learning_rate)\n",
    "        self.threshold = 2.0\n",
    "        self.num_of_epochs = number_of_epochs\n",
    "        self.is_hinge_loss = is_hinge_loss\n",
    "        self.hebbian_optimizer = Adam([Layer.hebbian_weights_layer_two, Layer.hebbian_weights_layer_one], lr=0.02)\n",
    "        \n",
    "        \n",
    "    def compute_hebbian_activity(self, labels, values, layer_num):\n",
    "        \n",
    "        \n",
    "        if layer_num == 0:\n",
    "            hebbian_value = torch.mm(values.pow(2) * self.layer_weights[layer_num], self.hebbian_weights_layer_one.T) * labels\n",
    "        else:\n",
    "            hebbian_value = torch.mm(values, self.hebbian_weights_layer_two.T) * labels\n",
    "        return hebbian_value.mean(1).cuda()\n",
    "    \n",
    "    \n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        normalized_input = input / (input.norm(2, 1, keepdim=True) + 1e-4)\n",
    "        output = torch.mm(normalized_input, self.weight.T) + self.bias.unsqueeze(0)\n",
    "        return self.activation(output)\n",
    "\n",
    "    def balanced_loss(self, positive_goodness, negative_goodness, alpha=8.0):\n",
    "        delta = positive_goodness - negative_goodness\n",
    "        per_instance_loss = torch.log(1 + torch.exp(-alpha * delta))\n",
    "        return per_instance_loss.mean()  \n",
    "\n",
    "\n",
    "    def exponential_hinge_loss(self, positive_goodness, negative_goodness, delta=1.0, is_second_phase=False):\n",
    "        if is_second_phase:\n",
    "            threshold = self.threshold * 2\n",
    "        else:\n",
    "            threshold = self.threshold\n",
    "        positive_loss = torch.exp(torch.clamp(delta - (positive_goodness - threshold), min=0)) - 1\n",
    "        negative_loss = torch.exp(torch.clamp(delta - (threshold - negative_goodness), min=0)) - 1\n",
    "        return torch.cat([positive_loss, negative_loss]).mean()\n",
    "\n",
    "\n",
    "\n",
    "    def plot_goodness(self, positive_goodness, negative_goodness):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(positive_goodness, label='Positive Goodness', color='b')\n",
    "        plt.plot(negative_goodness, label='Negative Goodness', color='r')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Goodness Value')\n",
    "        plt.title('Change in Goodness During Training')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def train_layer(self, positive_input, negative_input, layer_num):\n",
    "        positive_goodness_history = []\n",
    "        negative_goodness_history = []\n",
    "        \n",
    "        for _ in tqdm(range(self.num_of_epochs)):\n",
    "            positive_output = self.forward(positive_input)  # Shape: [batch_size, 500]\n",
    "            negative_output = self.forward(negative_input)\n",
    "            \n",
    "            # First Layer\n",
    "            if layer_num == 0:\n",
    "                # positive_goodness = (torch.mm(Network.positive_labels, Layer.hebbian_weights_layer_one_zeroth) * positive_output + torch.mm(Network.positive_labels, Layer.hebbian_weights_layer_one) * positive_output.pow(2) + torch.mm(Network.positive_labels, Layer.hebbian_weights_layer_one_quadra) * positive_output.pow(4) + torch.mm(Network.positive_labels, Layer.hebbian_weights_layer_one_third) * positive_output.pow(3)).mean(1)\n",
    "                positive_goodness = (torch.mm(Network.positive_labels, Layer.hebbian_weights_layer_one) * positive_output.pow(2)).mean(1)\n",
    "                # negative_goodness = (torch.mm(Network.negative_labels, Layer.hebbian_weights_layer_one_zeroth) * negative_output + torch.mm(Network.negative_labels, Layer.hebbian_weights_layer_one) * negative_output.pow(2) + torch.mm(Network.negative_labels, Layer.hebbian_weights_layer_one_quadra) * negative_output.pow(4) + torch.mm(Network.negative_labels, Layer.hebbian_weights_layer_one_third) * negative_output.pow(3)).mean(1)\n",
    "                negative_goodness = (torch.mm(Network.negative_labels, Layer.hebbian_weights_layer_one) * negative_output.pow(2)).mean(1)\n",
    "            else:\n",
    "                # Second Layer\n",
    "                # positive_goodness = (torch.mm(Network.positive_labels, Layer.hebbian_weights_layer_two_zeroth) * positive_output + torch.mm(Network.positive_labels, Layer.hebbian_weights_layer_two) * positive_output.pow(2) + torch.mm(Network.positive_labels, Layer.hebbian_weights_layer_two_quadra) * positive_output.pow(4) + torch.mm(Network.positive_labels, Layer.hebbian_weights_layer_two_third) * positive_output.pow(3)).mean(1)\n",
    "                positive_goodness = (torch.mm(Network.positive_labels, Layer.hebbian_weights_layer_two) * positive_output.pow(2)).mean(1)\n",
    "                # negative_goodness = (torch.mm(Network.negative_labels, Layer.hebbian_weights_layer_two_zeroth) * positive_output + torch.mm(Network.negative_labels, Layer.hebbian_weights_layer_two) * negative_output.pow(2) + torch.mm(Network.negative_labels, Layer.hebbian_weights_layer_two_quadra) * negative_output.pow(4) + torch.mm(Network.negative_labels, Layer.hebbian_weights_layer_two_third) * negative_output.pow(3)).mean(1)\n",
    "                negative_goodness = (torch.mm(Network.negative_labels, Layer.hebbian_weights_layer_two) * negative_output.pow(2)).mean(1)\n",
    "            \n",
    "            positive_goodness_history.append(positive_goodness.mean().item())\n",
    "            negative_goodness_history.append(negative_goodness.mean().item())\n",
    "\n",
    "            if self.is_hinge_loss:\n",
    "                loss = self.exponential_hinge_loss(positive_goodness, negative_goodness)\n",
    "            else:\n",
    "                loss = self.balanced_loss(positive_goodness, negative_goodness)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            self.hebbian_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.hebbian_optimizer.step()\n",
    "        \n",
    "        self.plot_goodness(positive_goodness_history, negative_goodness_history)\n",
    "        return self.forward(positive_input).detach(), self.forward(negative_input).detach()\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "    # hebbian_weights = nn.Parameter(torch.ones(10, 774).cuda())\n",
    "    positive_labels = []\n",
    "    negative_labels= []\n",
    "    def __init__(self, dimension_configs):\n",
    "        super().__init__()\n",
    "        self.layers = []\n",
    "        for i in range(len(dimension_configs) - 1):\n",
    "            self.layers += [Layer(dimension_configs[i], dimension_configs[i + 1]).cuda()]\n",
    "            \n",
    "    def balanced_loss(self, positive_goodness, negative_goodness, alpha=4.0):\n",
    "        delta = positive_goodness - negative_goodness\n",
    "        per_instance_loss = torch.log(1 + torch.exp(-alpha * delta))\n",
    "        return per_instance_loss.mean()  \n",
    "\n",
    "    def mark_data(self, data, label):\n",
    "        marked_data = data.clone().cuda()\n",
    "        marked_data[:, :10] = 0\n",
    "        marked_data[torch.arange(marked_data.size(0)), label] = 1\n",
    "        return marked_data\n",
    "    \n",
    "    \n",
    "    def predict(self, input_data):\n",
    "        goodness_per_label = []\n",
    "        for label in range(10):\n",
    "            marked_data = self.mark_data(input_data, label)\n",
    "            # hebbian_weight_one_zeroth = Layer.hebbian_weights_layer_one_zeroth[label, :]\n",
    "            # hebbian_weight_two_zeroth = Layer.hebbian_weights_layer_two_zeroth[label, :]\n",
    "            hebbian_weight_one = Layer.hebbian_weights_layer_one[label, :]\n",
    "            hebbian_weight_two = Layer.hebbian_weights_layer_two[label, :]\n",
    "            # hebbian_weight_one_third = Layer.hebbian_weights_layer_one_third[label, :]\n",
    "            # hebbian_weight_two_third = Layer.hebbian_weights_layer_two_third[label, :]\n",
    "            # hebbian_weight_one_quadra = Layer.hebbian_weights_layer_one_quadra[label, :]\n",
    "            # hebbian_weight_two_quadra = Layer.hebbian_weights_layer_two_quadra[label, :]\n",
    "            goodness = []\n",
    "            for layer_num, layer in enumerate(self.layers):\n",
    "                marked_data = layer(marked_data)\n",
    "                if layer_num == 0:\n",
    "                    goodness_value = (marked_data.pow(2) * hebbian_weight_one).mean(1)\n",
    "                else:\n",
    "                    goodness_value = (marked_data.pow(2) * hebbian_weight_two).mean(1)\n",
    "                goodness.append(goodness_value)\n",
    "\n",
    "            goodness_per_label.append(torch.sum(torch.stack(goodness), dim=0).unsqueeze(1))\n",
    "        goodness_per_label = torch.cat(goodness_per_label, 1)\n",
    "        return goodness_per_label.argmax(dim=1)\n",
    "    \n",
    "    def compute_hebbian_activity(self, values):\n",
    "        labels = values[:, :10].cuda()\n",
    "        hebbian = values[:, 10:].cuda()\n",
    "        hebbian_value = torch.mm(hebbian, self.hebbian_weights.T) * labels\n",
    "        return hebbian_value.mean(1).cuda()\n",
    "    \n",
    "    \n",
    "    def train_network(self, positive_goodness, negative_goodness):\n",
    "        \n",
    "        # positive_mean_values = []\n",
    "        # negative_mean_values = []\n",
    "        # \n",
    "        # for _ in tqdm(range(number_of_epochs)):\n",
    "        #     positive_labels = positive_goodness[:, :10].cuda()\n",
    "        #     positive_hebbian = positive_goodness[:, 10:].cuda()\n",
    "        #     positive_hebbian_value = torch.mm(positive_hebbian, self.hebbian_weights.T.pow(2)) * positive_labels\n",
    "        #     positive_mean_value = positive_hebbian_value.mean(1).cuda()\n",
    "        #     positive_mean_values.append(positive_mean_value.mean().item())  # Store average for plotting\n",
    "        # \n",
    "        #     negative_labels = negative_goodness[:, :10].cuda()\n",
    "        #     negative_hebbian = negative_goodness[:, 10:].cuda()\n",
    "        #     negative_hebbian_value = torch.mm(negative_hebbian, self.hebbian_weights.T.pow(2)) * negative_labels\n",
    "        #     negative_mean_value = negative_hebbian_value.mean(1).cuda()\n",
    "        #     negative_mean_values.append(negative_mean_value.mean().item())  # Store average for plotting\n",
    "        # \n",
    "        #     loss = self.balanced_loss(positive_mean_value, negative_mean_value)\n",
    "        #     self.hebbian_optimizer.zero_grad()\n",
    "        #     loss.backward()\n",
    "        #     self.hebbian_optimizer.step()\n",
    "        # \n",
    "        # plt.figure(figsize=(10, 5))\n",
    "        # plt.plot(positive_mean_values, label='Positive Mean Value', color='blue')\n",
    "        # plt.plot(negative_mean_values, label='Negative Mean Value', color='red')\n",
    "        # plt.title('Change in Mean Values Over Epochs')\n",
    "        # plt.xlabel('Epochs')\n",
    "        # plt.ylabel('Mean Value')\n",
    "        # plt.legend()\n",
    "        # plt.grid(True)\n",
    "        # plt.show()\n",
    "        # \n",
    "        goodness_pos, goodness_neg = positive_goodness, negative_goodness\n",
    "        positive_labels = goodness_pos[:, :10]\n",
    "        negative_labels = negative_goodness[:, :10]\n",
    "\n",
    "        Network.positive_labels = nn.Parameter(positive_labels.cuda())\n",
    "        Network.negative_labels = nn.Parameter(negative_labels.cuda())\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            print('Training Layer', i, '...')\n",
    "            goodness_pos, goodness_neg = layer.train_layer(goodness_pos, goodness_neg, i)\n",
    "            \n",
    "\n",
    "\n",
    "def load_CIFAR10_data(train_batch_size=30000, test_batch_size=6000):\n",
    "    data_transformation = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)), \n",
    "        Lambda(lambda x: torch.flatten(x))\n",
    "    ])\n",
    "\n",
    "    training_data_loader = DataLoader(\n",
    "        CIFAR10('./data/', train=True, download=True, transform=data_transformation),\n",
    "        batch_size=train_batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    testing_data_loader = DataLoader(\n",
    "        CIFAR10('./data/', train=False, download=True, transform=data_transformation),\n",
    "        batch_size=test_batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    return training_data_loader, testing_data_loader\n",
    "\n",
    "def load_FashionMNIST_data(train_batch_size=60000, test_batch_size=10000):\n",
    "    data_transformation = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize((0.2860,), (0.3530,)),\n",
    "        Lambda(lambda x: torch.flatten(x))\n",
    "    ])\n",
    "\n",
    "    training_data_loader = DataLoader(\n",
    "        FashionMNIST('./data/', train=True, download=True, transform=data_transformation),\n",
    "        batch_size=train_batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    testing_data_loader = DataLoader(\n",
    "        FashionMNIST('./data/', train=False, download=True, transform=data_transformation),\n",
    "        batch_size=test_batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    return training_data_loader, testing_data_loader\n",
    "\n",
    "def load_MNIST_data(train_batch_size=50000, test_batch_size=10000):\n",
    "    data_transformation = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize((0.1307,), (0.3081,)),\n",
    "        Lambda(lambda x: torch.flatten(x))\n",
    "    ])\n",
    "\n",
    "    training_data_loader = DataLoader(\n",
    "        MNIST('./data/', train=True, download=True, transform=data_transformation),\n",
    "        batch_size=train_batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    testing_data_loader = DataLoader(\n",
    "        MNIST('./data/', train=False, download=True, transform=data_transformation),\n",
    "        batch_size=test_batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    return training_data_loader, testing_data_loader\n",
    "\n",
    "\n",
    "def create_positive_data(data, label):\n",
    "    positive_data = data.clone()\n",
    "    positive_data[:, :10] = 0.0\n",
    "\n",
    "    for i in range(positive_data.shape[0]):\n",
    "        positive_data[i][label[i]] = 1.0\n",
    "\n",
    "    return positive_data\n",
    "\n",
    "\n",
    "def create_negative_data(data, label, seed=None):\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "\n",
    "    negative_data = data.clone()\n",
    "    negative_data[:, :10] = 0.0\n",
    "\n",
    "    for i in range(negative_data.shape[0]):\n",
    "        possible_answers = list(range(10))\n",
    "        possible_answers.remove(label[i])\n",
    "        false_label = random.choice(possible_answers)\n",
    "        negative_data[i][false_label] = 1.0\n",
    "\n",
    "    return negative_data\n",
    "\n",
    "\n",
    "def prepare_data():\n",
    "    torch.manual_seed(4321)\n",
    "    training_data_loader, testing_data_loader = load_MNIST_data()\n",
    "\n",
    "    training_data, training_data_label = next(iter(training_data_loader))\n",
    "\n",
    "    testing_data, testing_data_label = next(iter(testing_data_loader))\n",
    "    testing_data, testing_data_label = testing_data.cuda(), testing_data_label.cuda()\n",
    "\n",
    "    print(f\"Training Data: \", training_data)\n",
    "    print(f\"Training Data Label: \", training_data_label)\n",
    "\n",
    "    training_data, training_data_label = training_data.cuda(), training_data_label.cuda()\n",
    "\n",
    "    positive_data = create_positive_data(training_data, training_data_label)\n",
    "    print(f\"Positive Data: \", positive_data)\n",
    "\n",
    "    negative_data = create_negative_data(training_data, training_data_label, seed=1234)\n",
    "    print(f\"Negative Data: \", negative_data)\n",
    "\n",
    "    return positive_data, negative_data, training_data, training_data_label, testing_data, testing_data_label\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.manual_seed(1234)\n",
    "    positive_data, negative_data, training_data, training_data_label, testing_data, testing_data_label = prepare_data()\n",
    "    network = Network([784, 500, 500]).cuda() #3072\n",
    "    network.train_network(positive_data, negative_data)\n",
    "\n",
    "    print(\"Training Accuracy: \", network.predict(training_data).eq(training_data_label).float().mean().item())\n",
    "\n",
    "    print(\"Testing Accuracy: \", network.predict(testing_data).eq(testing_data_label).float().mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d8aedab7475e0bf7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
